---
title: "Relaxing MaxEnt: Tense"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lme4)
library(tidymodels)
library(themis)
library(patchwork)
library(ggrepel)
library(ggridges)
library(here)

# Just some color scales for visualization consistency
te_scale = scale_fill_manual(values=c('#be29ec', '#d896ff', '#efbbff', '#ae0001', '#008080', '#66b2b2', '#b2d8d8'))
ter_scale = scale_fill_manual(values=rev(c('#be29ec', '#d896ff', '#efbbff', '#ae0001', '#008080', '#66b2b2', '#b2d8d8')))

library(reticulate)
use_python("/usr/bin/python3", required = TRUE)
```

```{python config, echo=F}
import sys
sys.path.append(r.here())
import numpy as np
import pandas
from collections import namedtuple
from ibhelpers import *
from scipy.spatial import distance
```

# Tense

## Meaning

In this analysis, we will use the information bottleneck method to define an optimal frontier, score attested languages, score possible languages and check whether distance to the pareto-frontier is a better predictor of whether a language is attested than either complexity or information loss alone.

### Assumptions

The first thing we do is formalize our assumptions.

```{python tense_specification}

items = ['a', 'b', 'c', 'r', 'x', 'y', 'z']

# probabilities from Geoff's dissertation from Google N-grams (1985)
allp =  np.array([0.1034, 0.0795, 0.1839, 0.6183, 0.0074, 0.0048, 0.0028])
pastprops = allp[0:3]/np.sum(allp[0:3])
futprops = allp[4:7]/np.sum(allp[4:7])
duboisprobs = np.array([27.4, 47.5, 25.1]) # From Twitter Corpus
duboisprobs = duboisprobs / np.sum(duboisprobs)
p_x = np.concatenate( [duboisprobs[0] * pastprops, duboisprobs[[1]], duboisprobs[2]*futprops ] )
p_x = p_x / np.sum(p_x)

eps = 0.01
q0 = (1 - eps) * np.eye(7) + eps * np.ones((7, 7))

kap = 0.5
lam = 0.1

p_xGy = np.array(
    [[1, kap, kap ** 2, lam * kap ** 2, lam ** 2 * kap ** 2, lam ** 2 * kap ** 3, lam ** 2 * kap ** 4],
     [kap, 1, kap, lam * kap, lam ** 2 * kap, lam ** 2 * kap ** 2, lam ** 2 * kap ** 3],
     [kap ** 2, kap, 1, lam, lam ** 2, lam ** 2 * kap, lam ** 2 * kap ** 2],
     [lam * kap ** 2, lam * kap, lam, 1, lam, lam * kap, lam * kap ** 2],
     [lam ** 2 * kap ** 2, lam ** 2 * kap, lam ** 2, lam, 1, kap, kap ** 2],
     [lam ** 2 * kap ** 3, lam ** 2 * kap ** 2, lam ** 2 * kap, lam * kap, kap, 1, kap],
     [lam ** 2 * kap ** 4, lam ** 2 * kap ** 3, lam ** 2 * kap ** 2, lam * kap ** 2, kap ** 2, kap, 1]])

p_mGs = p_xGy / np.sum(p_xGy, axis=0)
p_xGy = p_xGy / p_xGy.sum(axis=1, keepdims=True)
p_xy = p_xGy * p_x[:, np.newaxis]
p_xy = p_xy / np.sum(p_xy)

```

Let's plot these assumptions.

```{r tense_fig1}

tense_prior = data.frame(x=c('R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz'),
                         p=py$p_x) %>%
    mutate(x = fct_relevel(x, 'R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz'))

tense_px = tense_prior %>%
    ggplot(aes(x, p, fill=x)) +
    geom_bar(stat='identity') +
    theme_classic(base_size = 18) +
    ylab('Prior Probability') +
    xlab('Speaker Distribution') +
    theme(plot.title = element_text(hjust = 0.5)) +
    coord_cartesian(ylim = c(0, 0.75)) +
    te_scale +
    scale_x_discrete(labels=c(expression(s[a]), expression(s[b]), expression(s[c]), expression(s[r]), expression(s[x]), expression(s[y]), expression(s[z]))) +
    guides(fill=F)
tense_px

tense_meaning = py$p_xGy %>%
    as.data.frame() %>%
    rename(`s[a]` = V1, `s[b]` = V2, `s[c]` = V3, `s[r]` = V4, `s[x]` = V5, `s[y]` = V6, `s[z]` = V7) %>%
    mutate(x=c('R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz')) %>%
    gather(goal, p, `s[a]`:`s[z]`) %>%
    mutate(goal = fct_relevel(goal, 's[z]', 's[y]', 's[x]', 's[r]', 's[c]', 's[b]', 's[a]'),
           x=fct_relevel(x, 'R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz'))

tense_pxy = tense_meaning %>%
    ggplot(aes(x=x, y=p, group=goal)) +
    facet_grid(goal~., scales = 'free_y', switch = 'y', as.table = F, labeller = label_parsed) +
    geom_hline(yintercept = 0) +
    geom_bar(stat='identity', aes(fill=goal), alpha=0.8) +
    theme_classic(base_size = 18) +
    guides(fill=F) +
    ggtitle('Tense') +
    ylab('Speaker Distribution') +
    xlab('World State') +
    ter_scale +
    theme(axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          axis.line.y = element_blank())
tense_pxy

```

### The Pareto Frontier

Now let's run the Information Bottleneck Method

```{python tense_ib}
# trace out optimal frontier
q0 = q0 / q0.sum(axis=1, keepdims=True)
betas = np.array([2.0 ** x for x in np.arange(5, 0, -0.001)])
focalbeta = 5.3

q, beta, ibscores, qresult, qseq, qseqresults, allqs = fit_ib(p_xy, q0, focalbeta, betas, verbose=1)

# create data frames for plotting and analysis
ib_scores_df = pandas.DataFrame(np.array(ibscores), columns = ['rate', 'distortion', 'elen'])
ib_scores_df['beta'] = betas
ib_scores_df['q'] = allqs
ib_scores_df['Wn'] = [mergecols(q).shape[1] for q in ib_scores_df['q']]

# The structural phase transitions along the pareto frontier
stochSys = []
for i, q in enumerate(zip(qseq, qseqresults)):
    for w in mergecols(q[0]).transpose():
        stochSys.append([len(qseq)-i, q[1][0], q[1][1]] + list(w))

stochSys = pandas.DataFrame(data=np.array(stochSys), columns = ['n', 'rate', 'distortion'] + items)

```

```{python frontier_metrics}

# compute distance from optimal frontier
def fd(asys, ibscores):
    mind = distance.cdist([[asys['rate'], asys['distortion']]], ibscores[['rate', 'distortion']]).min()
    return mind


def gNID_d(asys, paretoQs, betas, pX):
    mind = np.zeros((len(asys), len(paretoQs)))
    for li in range(len(asys)):
        for qi, q in enumerate(paretoQs):
            mind[li, qi] = gNID(asys.iloc[li]['q'], q, pX)
    return np.argmin(mind, axis=1), np.min(mind, axis=1), betas[np.argmin(mind, axis=1)]


```

### Attested Languages

First we load and pre-process

```{python tense_channels}

bd =  pandas.read_csv('../Data/Tense_Meanings.csv')


def present(x):
    '''
      Is this distinction present in a language?
    '''
    x =  str(x)
    x = str.split(x, ';')[0]
    return  (x ==  'i' or x  == 'p')


def extract_cats(td):
    '''
      Map a present category into an interval of the domain?
    '''
    cats = []
    if present(td.past):
        cats.append([0,1,2])
    if present(td.present):
        cats.append([3])
    if present(td.future):
        cats.append([4,5,6])
    if present(td.immediate_past):
        cats.append([2])
    if present(td.recent_past):
        cats.append([1])
    if present(td.remote_past):
        cats.append([0])
    if present(td.immediate_future):
        cats.append([4])
    if present(td.remote_future):
        cats.append([5,6])
    if present(td.non_past):
        cats.append([3,4,5,6])
    if present(td.non_future):
        cats.append([0,1,2,3])
    if present(td.pre_hodiernal):
        cats.append([0,1])
    if present(td.past_wo_remote):
        cats.append([1,2])
    return cats


ef = extract_cats
n = 7

bd['categories'] = bd.apply(ef,  axis=1)
bd = bd[['Language', 'Family', 'Source', 'categories']]


def q2Fq(q):
    nqA = np.zeros(q.shape)
    biasA = [1, 0.25, 0.75]
    nqB = np.zeros(q.shape)
    biasB = [1, 0.75, 0.25]
    k = 1
    for i, sem in enumerate(q):
      for j, w in enumerate(sem):
        if w == 1:
          nqA[i, j] = 1.0
          nqB[i, j] = 1.0
        elif w == 0.5 and j < 5:
          nqA[i, j] = biasA.pop()
          nqB[i, j] = biasB.pop()
        elif w > 0:
          if q.shape[0] - i > k:
            k = q.shape[0] - i - 1
          nqA[i, j] = biasA[-1] / k
          nqB[i, j] = biasB[-1] / k
    return (nqA.T / nqA.sum(axis=1)).T, (nqB.T / nqB.sum(axis=1)).T


def unpackoutputs(row):
    # if multiple words for category assume uniform distribution for now -- revisit  later
    row['q'], row['lens']= cats2q(row['categories'], n)
    row['q1'], row['q2'] = q2Fq(row['q'])
    return row

bd = bd.apply(unpackoutputs, axis=1)
bd['ind'] = range(bd.shape[0])


```

Then we score.

```{python tense_attested}
attested_data = bd
attested_data = attested_data.dropna()
attested_data['rate'] = 0
attested_data['distortion'] = 0
attested_data['exp_len'] = 0
attested_data['lab'] = ''
attested_perm_zc_scores= []
npitems = np.array(items)

for i in attested_data['ind']:
    q = attested_data.loc[i,'q2']
    nat = naturalness(q, p_mGs)
    result = score_q_kl(p_xy, q)
    attested_data.loc[i,  'rate']  = result.rate
    attested_data.loc[i,  'distortion'] = result.distortion
    attested_data.loc[i,  'naturalness'] = np.sum(nat)
    lens = attested_data.loc[i, 'lens']
    attested_data.loc[i, 'exp_len'] = exp_len(q, p_x, lens)
    nc = len(attested_data.loc[i, 'categories'])
    attested_data.loc[i,  'n'] = nc
    attested_data.loc[i,  'lab'] = zmlabel(attested_data.loc[i, 'categories'], attested_data.loc[i,'lens'], npitems)
    attested_data.loc[i,  'zlab'] = zlabel(attested_data.loc[i, 'categories'], npitems)
    for j in np.arange(-1, nc):
        l  = make_lens(j, nc, n)
        e_len = exp_len(q, p_x, l)
        attested_perm_zc_scores.append( (result.rate, result.distortion, e_len))

attested_data['frontier_dist'] = attested_data.apply(fd,  args = (ib_scores_df,), axis=1)
attested_data['rate_round'] = np.round(attested_data['rate'], 4)
attested_data['distortion_round'] = np.round(attested_data['distortion'], 4)
attested_data['frontier_dist_round'] = np.round(attested_data['frontier_dist'], 4)
attested_data['exp_len_round'] = np.round(attested_data['exp_len'], 4)
_, attested_data['gNID'], attested_data['gNID_beta']  = gNID_d(attested_data, allqs, betas, p_x)
attested_data['Wn'] = [mergecols(q).shape[1] for q in attested_data['q']]

attested_perm_zc_df = pandas.DataFrame(np.array(attested_perm_zc_scores), columns = ['rate', 'distortion', 'exp_len'])

# Let's do a quick reformat for plotting
channels = []
for i in attested_data['ind']:
    q = attested_data.loc[i, 'q2']
    for w in mergecols(q).transpose():
        channels.append([attested_data.loc[i, 'Language'], attested_data.loc[i, 'rate'], attested_data.loc[i, 'distortion'], attested_data.loc[i, 'gNID_beta']] + list(w))

channels = pandas.DataFrame(data=np.array(channels), columns = ['Language', 'rate', 'distortion', 'beta'] + items)

# Save the partial outputs
attested_data.to_csv('../Output/Tense/attested_q2.csv')
attested_perm_zc_df.to_csv('../Output/Tense/attestedLen_q2.csv')
channels.to_csv('../Output/Tense/channels_q2.csv')

```

```{python tense_attested_q1}
attested_data = bd
attested_data = attested_data.dropna()
attested_data['rate'] = 0
attested_data['distortion'] = 0
attested_data['exp_len'] = 0
attested_data['lab'] = ''
attested_perm_zc_scores= []
npitems = np.array(items)

for i in attested_data['ind']:
    q = attested_data.loc[i,'q1']
    nat = naturalness(q, p_mGs)
    result = score_q_kl(p_xy, q)
    attested_data.loc[i,  'rate']  = result.rate
    attested_data.loc[i,  'distortion'] = result.distortion
    attested_data.loc[i,  'naturalness'] = np.sum(nat)
    lens = attested_data.loc[i, 'lens']
    attested_data.loc[i, 'exp_len'] = exp_len(q, p_x, lens)
    nc = len(attested_data.loc[i, 'categories'])
    attested_data.loc[i,  'n'] = nc
    attested_data.loc[i,  'lab'] = zmlabel(attested_data.loc[i, 'categories'], attested_data.loc[i,'lens'], npitems)
    attested_data.loc[i,  'zlab'] = zlabel(attested_data.loc[i, 'categories'], npitems)
    for j in np.arange(-1, nc):
        l  = make_lens(j, nc, n)
        e_len = exp_len(q, p_x, l)
        attested_perm_zc_scores.append( (result.rate, result.distortion, e_len))

attested_data['frontier_dist'] = attested_data.apply(fd,  args = (ib_scores_df,), axis=1)
attested_data['rate_round'] = np.round(attested_data['rate'], 4)
attested_data['distortion_round'] = np.round(attested_data['distortion'], 4)
attested_data['frontier_dist_round'] = np.round(attested_data['frontier_dist'], 4)
attested_data['exp_len_round'] = np.round(attested_data['exp_len'], 4)
_, attested_data['gNID'], attested_data['gNID_beta']  = gNID_d(attested_data, allqs, betas, p_x)
attested_data['Wn'] = [mergecols(q).shape[1] for q in attested_data['q']]

attested_perm_zc_df = pandas.DataFrame(np.array(attested_perm_zc_scores), columns = ['rate', 'distortion', 'exp_len'])

# Let's do a quick reformat for plotting
channels = []
for i in attested_data['ind']:
    q = attested_data.loc[i, 'q1']
    for w in mergecols(q).transpose():
        channels.append([attested_data.loc[i, 'Language'], attested_data.loc[i, 'rate'], attested_data.loc[i, 'distortion'], attested_data.loc[i, 'gNID_beta']] + list(w))

channels = pandas.DataFrame(data=np.array(channels), columns = ['Language', 'rate', 'distortion', 'beta'] + items)

# Save the partial outputs
attested_data.to_csv('../Output/Tense/attested_q1.csv')
attested_perm_zc_df.to_csv('../Output/Tense/attestedLen_q1.csv')
channels.to_csv('../Output/Tense/channels_q1.csv')

```

### Possible Systems

Let's generate and score some possible systems

```{python tense_possible}
# deterministic systems with item labels
detsystems = list(partition(items))
# deterministic systems with item indices
iteminds=list(range(len(items)))
detsysteminds = list(partition(iteminds))

# score deterministic systems
ds_scores = []
ds_scores_explen = []
dqs = []
for ds in detsysteminds:
    q = partition2q(ds)
    dqs.append(q)
    nat = naturalness(q, p_mGs)
    result = score_q_kl(p_xy, q)
    ds_scores.append( (result.rate, result.distortion, np.sum(nat)))
    nc = len(ds)
    for i in np.arange(-1, nc):
        l  = make_lens(i, nc, n)
        e_len = exp_len(q, p_x, l)
        ds_scores_explen.append( (result.rate, result.distortion, e_len))
        

# make labels for deterministic systems
syslabels = [partition_label(p) for p in detsystems]

ds_scores_df = pandas.DataFrame(np.array(ds_scores), columns = ['rate', 'distortion', 'naturalness'])
ds_scores_df['name'] = np.array(syslabels)
ds_scores_df['categories'] = np.array(detsysteminds)
ds_scores_df['q'] = dqs
ds_scores_df['n'] = ds_scores_df['categories'].apply(lambda x: len(x))
ds_scores_df['frontier_dist'] = ds_scores_df.apply(fd,  args = (ib_scores_df,), axis=1)
_, ds_scores_df['gNID'], ds_scores_df['gNID_beta']  = gNID_d(ds_scores_df, allqs, betas, p_x)
ds_scores_df['Wn'] = [mergecols(q).shape[1] for q in ds_scores_df['q']]

ds_scores_explen_df = pandas.DataFrame(np.array(ds_scores_explen), columns = ['rate', 'distortion', 'exp_len'])

```

### Putting it all Together

```{r}

tense_pareto = read.csv('../Output/Tense/pareto.csv')
tense_detSys = read.csv('../Output/Tense/detSys.csv')
tense_detSysLen = read.csv('../Output/Tense/detSysELen.csv')
tense_attested = read.csv('../Output/Tense/attested.csv')
tense_attested_q1 = read.csv('../Output/Tense/attested_q1.csv')
tense_attested_q2 = read.csv('../Output/Tense/attested_q2.csv')
tense_attestedLen = read.csv('../Output/Tense/attestedLen.csv')

tense_ag = tense_attested %>%
  group_by(zlab) %>%
  summarise(rate=mean(rate), 
            distortion=mean(distortion),
            frontier_dist=first(frontier_dist),
            N=n(), 
            lab=first(zlab),
            instance=first(Language))

tense_ag_q1 = tense_attested_q1 %>%
  group_by(zlab) %>%
  summarise(rate=mean(rate), 
            distortion=mean(distortion),
            frontier_dist=first(frontier_dist),
            N=n(), 
            lab=first(zlab),
            instance=first(Language))

tense_ag_q2 = tense_attested_q2 %>%
  group_by(zlab) %>%
  summarise(rate=mean(rate), 
            distortion=mean(distortion),
            frontier_dist=first(frontier_dist),
            N=n(), 
            lab=first(zlab),
            instance=first(Language))

tense_ag2 = tense_attested %>%
  group_by(Language) %>%
  mutate(q = paste0(unlist(q), collapse='')) %>%
  ungroup() %>%
  group_by(q) %>%
  summarise(rate=mean(rate),
            distortion=mean(distortion),
            N=n(),
            Wn=first(Wn),
            lab=first(q))

# Double Check Number of Languages
tense_attested %>% pull(Language) %>% unique() %>% length()

tense_frontier = tense_pareto %>%
  ggplot(aes(rate, distortion)) +
  geom_area(fill='grey60') +
  geom_line() +
  geom_point(data=tense_detSys, color='grey80') +
  geom_point(data=tense_ag, aes(size=N)) +
  geom_point(data=tense_ag_q1, color='red') +
  geom_point(data=tense_ag_q2, color='green') +
  #geom_text_repel(data=tense_ag %>% filter(N > 10), aes(label=lab)) +
  ylab('Information Loss') +
  xlab('Complexity') +
  theme_classic(base_size = 14) +
  ggtitle('Tense') +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_cartesian(ylim=c(0, 1), xlim=c(0, 2))
tense_frontier

tense_ag %>%
  mutate(d = frontier_dist * N) %>%
  pull(d) %>% sum

tense_ag_q1 %>%
  mutate(d = frontier_dist * N) %>%
  pull(d) %>% sum

tense_ag_q2 %>%
  mutate(d = frontier_dist * N) %>%
  pull(d) %>% sum


left_join(tense_ag %>%
  select(zlab, instance, q = frontier_dist),
  tense_ag_q1 %>%
    select(zlab, q1 = frontier_dist, instance)) %>%
  left_join(tense_ag_q2 %>%
    select(zlab, q2 = frontier_dist, instance)) %>%
  gather(channel, front_dist, q:q2) %>% 
  ggplot(aes(zlab, front_dist, color=channel)) +
  geom_point(size=3) +
  theme_bw() +
  coord_flip()
  
  
left_join(tense_ag %>%
            select(zlab, instance, q = rate),
          tense_ag_q1 %>%
            select(zlab, q1 = rate, instance)) %>%
  left_join(tense_ag_q2 %>%
              select(zlab, q2 = rate, instance)) %>%
  gather(channel, rate, q:q2) %>% 
  ggplot(aes(zlab, rate, color=channel)) +
  geom_point(size=3) +
  theme_bw() +
  coord_flip()


left_join(tense_ag %>%
            select(zlab, instance, q = distortion),
          tense_ag_q1 %>%
            select(zlab, q1 = distortion, instance)) %>%
  left_join(tense_ag_q2 %>%
              select(zlab, q2 = distortion, instance)) %>%
  gather(channel, distortion, q:q2) %>% 
  ggplot(aes(zlab, distortion, color=channel)) +
  geom_point(size=3) +
  theme_bw() +
  coord_flip()

library(tidymodels)
library(themis)

set.seed(1234)
```

Q1

```{r}

# Load data
ten_detSys = read.csv('../Output/Tense/detSys.csv')
ten_attested = read.csv('../Output/Tense/attested_q1.csv')

ten_systems = ten_attested %>% 
    select(rate_round, distortion_round, frontier_dist_round, gNID) %>% distinct() %>%
    full_join(ten_detSys %>% 
                  mutate(rate_round=round(rate,4), distortion_round=round(distortion,4)) %>% 
                  select(X, rate_round, distortion_round, front=frontier_dist, gNID)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Possible', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round)) 

table(ten_systems$System)

######## Formulas

# Complexity Formula
ten_rec_rate <- 
  recipe(System ~ rate_round, data = ten_systems) %>%
  step_rose(System)

# Information Loss Formula
ten_rec_dist <- 
  recipe(System ~ distortion_round, data = ten_systems) %>%
  step_rose(System)

# ParetoFront Formula
ten_rec_front <- 
  recipe(System ~ frontier_distance_round, data = ten_systems) %>%
  step_rose(System)


######## Model

ten_mod <-
  logistic_reg() %>%
  set_engine('glm')

# Workflow
ten_rose_wflw <- 
  workflow() %>% 
  add_model(ten_mod) 

######## Evaluation

cv_folds <- vfold_cv(ten_systems, strata = "System", repeats = 10)

logliklihood = function(...){ mn_log_loss(..., sum=TRUE)}
class(logliklihood) <- class(mn_log_loss)
attr(logliklihood, "direction") <- attr(mn_log_loss, "direction")

######## Fits

# Complexity
ten_rose_res_rate <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_rate), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_rate)

# Information Loss

ten_rose_res_dist <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_dist), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_dist)

# ParetoFront

ten_rose_res_front <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_front), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_front)

######## Log likelihoods

ten_rose_res_rate %>%
  collect_predictions() %>%
  mutate(logLike = ifelse(System=='Attested', log(.pred_Attested), log(.pred_Possible))) %>%
  pull(logLike) %>% sum

ten_rose_res_dist %>%
  collect_predictions() %>%
  mutate(logLike = ifelse(System=='Attested', log(.pred_Attested), log(.pred_Possible))) %>%
  pull(logLike) %>% sum

ten_rose_res_front %>%
  collect_predictions() %>%
  mutate(logLike = ifelse(System=='Attested', log(.pred_Attested), log(.pred_Possible))) %>%
  pull(logLike) %>% sum

```

Q2

```{r}
library(tidymodels)
library(themis)

set.seed(1234)
```

```{r}

# Load data
ten_detSys = read.csv('../Output/Tense/detSys.csv')
ten_attested = read.csv('../Output/Tense/attested_q2.csv')

ten_systems = ten_attested %>% 
    select(rate_round, distortion_round, frontier_dist_round, gNID) %>% distinct() %>%
    full_join(ten_detSys %>% 
                  mutate(rate_round=round(rate,4), distortion_round=round(distortion,4)) %>% 
                  select(X, rate_round, distortion_round, front=frontier_dist, gNID)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Possible', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round)) 

table(ten_systems$System)

######## Formulas

# Complexity Formula
ten_rec_rate <- 
  recipe(System ~ rate_round, data = ten_systems) %>%
  step_rose(System)

# Information Loss Formula
ten_rec_dist <- 
  recipe(System ~ distortion_round, data = ten_systems) %>%
  step_rose(System)

# ParetoFront Formula
ten_rec_front <- 
  recipe(System ~ frontier_distance_round, data = ten_systems) %>%
  step_rose(System)


######## Model

ten_mod <-
  logistic_reg() %>%
  set_engine('glm')

# Workflow
ten_rose_wflw <- 
  workflow() %>% 
  add_model(ten_mod) 

######## Evaluation

cv_folds <- vfold_cv(ten_systems, strata = "System", repeats = 10)

logliklihood = function(...){ mn_log_loss(..., sum=TRUE)}
class(logliklihood) <- class(mn_log_loss)
attr(logliklihood, "direction") <- attr(mn_log_loss, "direction")

######## Fits

# Complexity
ten_rose_res_rate <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_rate), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_rate)

# Information Loss

ten_rose_res_dist <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_dist), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_dist)

# ParetoFront

ten_rose_res_front <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_front), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_front)

######## Log likelihoods

ten_rose_res_rate %>%
  collect_predictions() %>%
  mutate(logLike = ifelse(System=='Attested', log(.pred_Attested), log(.pred_Possible))) %>%
  pull(logLike) %>% sum

ten_rose_res_dist %>%
  collect_predictions() %>%
  mutate(logLike = ifelse(System=='Attested', log(.pred_Attested), log(.pred_Possible))) %>%
  pull(logLike) %>% sum

ten_rose_res_front %>%
  collect_predictions() %>%
  mutate(logLike = ifelse(System=='Attested', log(.pred_Attested), log(.pred_Possible))) %>%
  pull(logLike) %>% sum

```

