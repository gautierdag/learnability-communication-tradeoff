---
title: "Number: Continuous Constraint"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/usr/bin/python3", required = TRUE)

library(tidyverse)
library(patchwork)
library(ggrepel)
library(ggridges)
library(tidymodels)
library(themis)

set.seed(1234)
```

```{r plot_guides}
nu_scale = scale_fill_gradient(low='#D3A625', high='#d34f25')
nur_scale = scale_fill_gradient(high='#D3A625', low='#d34f25')
```

# Number

Let's make some channel codes for existing languages.

```{python number_channel}
import numpy as np
from ibhelpers import *
from tense_specs import domain_spec
from scipy.spatial import distance
import pandas

datapath = '../Data/Number_Meanings.csv'
attested_data = pandas.read_csv(datapath)

q_b  =   np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])
q_sg =   np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
q_du =   np.array([0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
q_tr =   np.array([0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
q_pauc = np.array([0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0])
q_pauc2 = np.array([0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0])
q_pauc4 = np.array([0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0])
q_gpauc = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0])
q_pl =   np.array([0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])
q_pl3 =   np.array([0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])
q_pl6 =   np.array([0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0])
q_pl8 =  np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0])



def present(x):
    x =  str(x)
    x = str.split(x, ';')[0]
    return (x in ['a', 'b', 'c', 'd', 'e', 'f'])


def make_fine_channel(td):
    q = []
    if present(td.BARE):
        q.append(q_b)
    if present(td.SG):
        q.append(q_sg)
    if present(td.DU):
        q.append(q_du)
    if present(td.TR):
        q.append(q_tr)
    if present(td.PAUC2):
        q.append(q_pauc2)
    if present(td.PAUC3):
        q.append(q_pauc)
    if present(td.PAUC4):
        q.append(q_pauc4)
    if present(td.GPAUC):
        q.append(q_gpauc)
    if present(td.PL2):
        q.append(q_pl)
    if present(td.PL3):
        q.append(q_pl3)
    if present(td.PL6):
        q.append(q_pl6)
    if present(td.PL8):
        q.append(q_pl8)
    
    if len(q) < 10:
        fill = [[j/(11-len(q)) for j in q[-1]]]*(11-len(q))
        del q[-1]
        q.extend(fill)
        
    q = np.array(q).transpose()
    
    return q / q.sum(axis=1, keepdims=True)


def make_channel(td):
    q = []
    if present(td.BARE):
        q.append(q_b)
    if present(td.SG):
        q.append(q_sg)
    if present(td.DU):
        q.append(q_du)
    if present(td.TR):
        q.append(q_tr)
    if present(td.PAUC):
        q.append(q_pauc)
    if present(td.PL):
        q.append(q_pl)
    
    if len(q) < 10:
        fill = [[j/(11-len(q)) for j in q[-1]]]*(11-len(q))
        del q[-1]
        q.extend(fill)
        
    q = np.array(q).transpose()
    
    return q / q.sum(axis=1, keepdims=True)

attested_data['q'] = attested_data.apply(make_fine_channel,  axis=1)
attested_data = attested_data[['Language', 'Family', 'Source', 'q']]
attested_data['ind'] = range(attested_data.shape[0])


items = [str(x+1) for x in range(10)]

```

Now let's specify the meanings P(xy) and the need probability P(x).

```{python number_specification}

p_x = np.array([0.645, 0.161, 0.072, 0.040, 0.026, 0.018, 0.013, 0.010, 0.008, 0.006])

eps = 1e-5

# Sam's prior
p_xGy = np.array(    [[0.999,     0,     0,     0,     0,     0,     0,     0,     0,     0],
                     [0.034, 0.942, 0.024,     0,     0,     0,     0,     0,     0,     0],
                     [    0, 0.102,  0.81, 0.088,     0,     0,     0,     0,     0,     0],
                     [    0, 0.004, 0.162, 0.682,  0.15, 0.003,     0,     0,     0,     0],
                     [    0,     0, 0.014, 0.202, 0.579, 0.193, 0.012,     0,     0,     0],
                     [    0,     0, 0.002,  0.03, 0.224, 0.499, 0.217, 0.026, 0.001,     0],
                     [    0,     0,     0, 0.005, 0.047, 0.233, 0.437, 0.229, 0.044, 0.004],
                     [    0,     0,     0, 0.001,  0.01, 0.064, 0.235,  0.39, 0.232, 0.061],
                     [    0,     0,     0,     0, 0.003, 0.018, 0.082, 0.241, 0.364, 0.239],
                     [    0,     0,     0,     0, 0.001, 0.006, 0.031, 0.114, 0.283, 0.399]])


p_xGy += eps
p_xGy = p_xGy / p_xGy.sum(axis=1, keepdims=True)
p_xy = p_xGy * p_x[:, np.newaxis]
p_xy = p_xy / np.sum(p_xy)

p_mGs = p_xy / np.sum(p_xy, axis=0)

eps = 0.01
q0 = (1 - eps) * np.eye(10) + eps * np.ones((10, 10))

```

Plot those assumptions

```{r number_fig1}
num_meaning = py$p_xGy %>%
    as.data.frame() %>%
    rename(m_1 = V1, m_2 = V2, m_3 = V3, m_4 = V4, m_5 = V5,
           m_6 = V6, m_7 = V7, m_8 = V8, m_9 = V9, m_10 = V10) %>%
    mutate(x=1:10) %>%
    gather(goal, p, m_1:m_10) %>%
    mutate(goal = fct_relevel(goal, c('m_10', 'm_9', 'm_8', 'm_7', 'm_6', 'm_5', 'm_4', 'm_3', 'm_2', 'm_1')))

num_meaning = py$p_xGy %>%
    as.data.frame() %>%
    rename(`s[1]` = V1, `s[2]` = V2, `s[3]` = V3, `s[4]` = V4, `s[5]` = V5,
           `s[6]` = V6, `s[7]` = V7, `s[8]` = V8, `s[9]` = V9, `s[10]` = V10) %>%
    mutate(x=1:10) %>%
    gather(goal, p,`s[1]`:`s[10]`) %>%
    mutate(goal = fct_relevel(goal, c('s[10]', 's[9]', 's[8]', 's[7]', 's[6]', 's[5]', 's[4]', 's[3]', 's[2]', 's[1]')))

num_pxy = num_meaning %>%
    ggplot(aes(x=x, y=goal, group=goal, height=p)) +
    geom_density_ridges(stat='identity', aes(fill=as.numeric(goal)), 
                        alpha=0.8, scale=0.95) +
    theme_classic(base_size = 18) +
    guides(fill=F) +
    ggtitle('Number') +
    ylab('Meaning') +
    xlab('') +
    scale_x_continuous(breaks=1:10, label=1:10) +
    nur_scale +
    scale_y_discrete(labels=c(expression(m[10]), expression(m[9]), expression(m[8]), expression(m[7]), expression(m[6]), 
                              expression(m[5]), expression(m[4]), expression(m[3]), expression(m[2]), expression(m[1])))
num_pxy

num_pxy = num_meaning %>%
    ggplot(aes(x=x, y=p, group=goal)) +
  facet_grid(goal~., scales = 'free_y', switch = 'y', as.table = F, labeller = label_parsed) +
    geom_bar(stat='identity', aes(fill=as.numeric(goal)), 
                        alpha=0.8) +
    theme_classic(base_size = 18) +
    geom_hline(yintercept = 0) +
    guides(fill=F) +
    ggtitle('Number') +
    ylab('Speaker Distribution') +
    xlab('World State') +
    scale_x_continuous(breaks=1:10, label=1:10) +
    nur_scale +
    theme(axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          axis.line.y = element_blank())
num_pxy

num_prior = data.frame(x=1:10, p=py$p_x)

num_px = num_prior %>%
    ggplot(aes(x, p, fill=x)) +
    geom_bar(stat='identity') +
    theme_classic(base_size = 24) +
    ylab('Need Probability') +
    xlab('World State') +
    theme(plot.title = element_text(hjust = 0.5)) +
    coord_cartesian(ylim = c(0, 0.75)) +
    scale_x_continuous(breaks=1:10, labels=c(expression(m[1]), expression(m[2]), expression(m[3]), expression(m[4]), expression(m[5]), 
                              expression(m[6]), expression(m[7]), expression(m[8]), expression(m[9]), expression(m[10]))) +
    nu_scale +
    guides(fill=F)
num_px

num_px = num_prior %>%
    ggplot(aes(x, p, fill=x)) +
    geom_bar(stat='identity') +
    theme_classic(base_size = 18) +
    ylab('Prior Probability') +
    xlab('Speaker Distribution') +
    theme(plot.title = element_text(hjust = 0.5)) +
    coord_cartesian(ylim = c(0, 0.75)) +
    scale_x_continuous(breaks=1:10, labels=c(expression(s[1]), expression(s[2]), expression(s[3]), expression(s[4]), expression(s[5]), 
                              expression(s[6]), expression(s[7]), expression(s[8]), expression(s[9]), expression(s[10]))) +
    nu_scale +
    guides(fill=F)
num_px

num_pxy / num_px + plot_layout(heights = c(5, 1))

```

Let's score the attested systems

```{python number_attested}
attested_data['rate'] = 0
attested_data['distortion'] = 0

for i in attested_data['ind']:
    q = attested_data.loc[i, 'q']
    n = naturalness(q, p_mGs)
    result = score_q_kl(p_xy, q)
    attested_data.loc[i,  'rate'] = result.rate
    attested_data.loc[i,  'distortion'] = result.distortion
    attested_data.loc[i,  'naturalness'] = np.sum(n)

```

Let's generate some possible systems

Permutations

```{python}
from itertools import permutations


def q2cats(q):
    outs = []
    q = mergecols(q)
    for c in range(q.shape[1]):
        out = []
        for i, r in enumerate(q[:, c]):
            if r > 0:
                out.append(i)
        outs.append(out)
    return outs

def full_sorted(x):
    x = map(sorted, x)
    return sorted(x, key=lambda y: y[0])


def translate(system, perm):
    key = dict()
    for k, v in enumerate(perm):
        key[k] = v
    new_cats = []
    for cat in system:
        new_cat = []
        for o in cat:
            new_cat.append(key[o])
        new_cats.append(new_cat)
    return new_cats

perms = dict()
for p in permutations([0,1,2,3,4,5,6,7,8,9]):
    seen = set()
    for q in attested_data['q']:
        a = q2cats(q)
        if tuple(map(tuple, a)) not in seen:
            seen.add(tuple(map(tuple, a)))
            t = translate(a, p)
            if tuple(map(tuple, full_sorted(t))) not in perms.keys():
                perms[tuple(map(tuple, map(sorted, t)))] = t

# import pickle
# with open('permutations_number.pkl', 'wb') as f:
#   pickle.dump(perms, f)


def check_contig(key):
  tot = 0
  for categ in key:
    cat = list(categ)
    cat.sort()
    tally = 0
    for i, c in enumerate(cat):
      if c == cat[0]+i:
        tally += 1
    if tally == len(cat):
      tot += 1
  if tot == len(key):
    return True
  else:
    return False


ps_scores = []
ps_scores_explen = []
pqs = []
ps_names = []
ps_categories = []
for pk, ps in perms.items():
  if check_contig(pk):
    q = cats2q(ps, 10)[0]
    pqs.append(q)
    nat = naturalness(q, p_mGs)
    print(nat)
    result = score_q_kl(p_xy, q)
    print(result)
    ps_scores.append( (result.rate, result.distortion, np.sum(nat)) )
    ps_names.append(pk)
    ps_categories.append(ps)

```

Partitions

```{python number_possible}
# deterministic systems with item labels
detsystems = list(partition(items))
# deterministic systems with item indices
iteminds = list(range(len(items)))
detsysteminds = list(partition(iteminds))


# score deterministic systems
ds_scores = []
dqs = []
ds_categories = []
for ds in detsysteminds:
  if check_contig(ds):
    q = partition2q(ds)
    dqs.append(q)
    n = naturalness(q, p_mGs)
    result = score_q_kl(p_xy, q)
    print(ds, result, np.sum(n))
    ds_scores.append((result.rate, result.distortion, np.sum(n)))
    ds_categories.append(ds)

```

Calculate the frontier

```{python number_ib}
# trace out optimal frontier
q0 = q0 / q0.sum(axis=1, keepdims=True)
betas = np.array([2.0 ** x for x in np.arange(4, 0, -0.001)])
focalbeta = 1.

q, beta, ibscores, qresult, qseq, qseqresults, allqs = fit_ib(p_xy, q0, focalbeta, betas, verbose=1)
```

Calculate the distance metrics

```{python number_postProcess}
ib_scores_df = pandas.DataFrame(np.array(ibscores), columns = ['rate', 'distortion', 'elen'])
ib_scores_df['beta'] = betas
ib_scores_df['q'] = allqs
ib_scores_df['Wn'] = [mergecols(q).shape[1] for q in ib_scores_df['q']]


# compute distance from optimal frontier
def fd(asys, ibscores):
    mind = distance.cdist([[asys['rate'], asys['distortion']]], ibscores[['rate', 'distortion']]).min()
    return mind


def gNID_d(asys, paretoQs, betas, pX):
    mind = np.zeros((len(asys), len(paretoQs)))
    for li in range(len(asys)):
        for qi, q in enumerate(paretoQs):
            mind[li, qi] = gNID(asys.iloc[li]['q'], q, pX)
    return np.argmin(mind, axis=1), np.min(mind, axis=1), betas[np.argmin(mind, axis=1)]


attested_data['frontier_dist'] = attested_data.apply(fd,  args = (ib_scores_df,), axis=1)
attested_data['rate_round'] = np.round(attested_data['rate'], 4)
attested_data['distortion_round'] = np.round(attested_data['distortion'], 4)
attested_data['frontier_dist_round'] = np.round(attested_data['frontier_dist'], 4)
#_, attested_data['gNID'], attested_data['gNID_beta']  = gNID_d(attested_data, allqs, betas, p_x)
attested_data['Wn'] = [mergecols(q).shape[1] for q in attested_data['q']]

ds_scores_df = pandas.DataFrame(np.array(ds_scores), columns = ['rate', 'distortion', 'naturalness'])
ds_scores_df['categories'] = np.array(ds_categories)
ds_scores_df['q'] = dqs
ds_scores_df['n'] = ds_scores_df['categories'].apply(lambda x: len(x))
ds_scores_df['frontier_dist'] = ds_scores_df.apply(fd,  args = (ib_scores_df,), axis=1)
#ds_scores_df['gNID'] = pandas.read_csv('Number/detSys_gNIDs.csv')['gNID']
#ds_scores_df['gNID_beta'] = pandas.read_csv('Number/detSys_gNIDs.csv')['gNID_beta']
ds_scores_df['Wn'] = [mergecols(q).shape[1] for q in ds_scores_df['q']]

ps_scores_df = pandas.DataFrame(np.array(ps_scores), columns = ['rate', 'distortion', 'naturalness'])
ps_scores_df['name'] = np.array(ps_names)
ps_scores_df['categories'] = np.array(ps_categories)
ps_scores_df['q'] = pqs
ps_scores_df['n'] = ps_scores_df['categories'].apply(lambda x: len(x))
ps_scores_df['rate_round'] = np.round(ps_scores_df['rate'], 4)
ps_scores_df['distortion_round'] = np.round(ps_scores_df['distortion'], 4)
ps_scores_df['frontier_dist'] = ps_scores_df.apply(fd,  args = (ib_scores_df,), axis=1)
ps_scores_df['Wn'] = [mergecols(q).shape[1] for q in ps_scores_df['q']]

ds_scores_df.to_csv('../Output/Number/detSys_cont.csv')
ps_scores_df.to_csv('../Output/Number/permSys_cont.csv')

```

Let's plot the pareto-frontier

```{r number_fig2}
num_pareto = py$ib_scores_df
num_detSys = py$ds_scores_df
num_permSys = py$ps_scores_df
num_attested = py$attested_data

num_ag = num_attested %>%
    group_by(Language) %>%
    mutate(q = paste0(unlist(q), collapse='')) %>%
    ungroup() %>%
    group_by(q) %>%
    summarise(rate=mean(rate),
              distortion=mean(distortion),
              N=n(),
              lab=first(q))

num_ag2 = num_attested %>%
    group_by(Language) %>%
    mutate(q = paste0(unlist(q), collapse='')) %>%
    ungroup() %>%
    group_by(q) %>%
    summarise(rate=mean(rate),
              distortion=mean(distortion),
              N=n(),
              Wn=first(Wn),
              lab=first(q))

num_attested %>% nrow()

num_frontier = num_pareto %>%
  ggplot(aes(rate, distortion)) +
  geom_area(fill='grey60') +
  geom_line() +
  geom_point(data=num_detSys %>% mutate(rate = round(rate, 2), distortion = round(distortion, 2)) %>% select(rate, distortion) %>% distinct(), 
    color='grey80') +
  geom_point(data=num_permSys %>% mutate(rate = round(rate, 2), distortion = round(distortion, 2)) %>% select(rate, distortion) %>% distinct(),
             color='darkorchid1', alpha=0.3) +
  geom_point(data=num_ag, aes(size=N)) +
  ylab('Information Loss') +
  xlab('Complexity') +
  theme_classic(base_size = 14) +
  ggtitle('Number') +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_cartesian(ylim=c(0, 1.5), xlim=c(0, 1.8))
num_frontier

num_frontier_simp = num_detSys %>%
  mutate(distortion = round(distortion, 2)) %>%
  select(Wn, distortion) %>%
  distinct() %>%
  ggplot(aes(Wn, distortion)) +
  geom_point(color='grey80') +
  geom_point(data=num_permSys %>% mutate(distortion = round(distortion, 2)) %>% select(Wn, distortion) %>% distinct(),
             color='darkorchid1', alpha=0.3) +
  geom_point(data=num_ag2, aes(size=N)) +
  ylab('Information Loss') +
  xlab('Inventory Complexity') +
  theme_classic(base_size = 14) +
  ggtitle('Number') +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks=1:10)
num_frontier_simp

num_systems = num_attested %>% 
    filter(Language != 'Piraha', Language != 'Kawi') %>%
    select(rate_round, distortion_round, frontier_dist_round, gNID) %>% distinct() %>%
    full_join(num_detSys %>% filter(rate > 0) %>%
                  mutate(rate_round=round(rate,4), distortion_round=round(distortion,4)) %>% 
                  select(rate_round, distortion_round, front=frontier_dist, gNID)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Possible', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round))

num_psystems = num_attested %>% 
    filter(Language != 'Piraha', Language != 'Kawi') %>%
    select(rate_round, distortion_round, frontier_dist_round, gNID) %>% distinct() %>%
    full_join(num_permSys %>% filter(rate > 0) %>%
                  mutate(rate_round=round(rate,4), distortion_round=round(distortion,4)) %>% 
                  select(rate_round, distortion_round, front=frontier_dist)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Permuted', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round))

num_front = num_systems %>%
    ggplot(aes(x=System, y=frontier_distance_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Distance to Pareto-Front') +
    xlab('') +
    theme_classic() +
    coord_flip()
num_front

num_pfront = num_psystems %>%
    ggplot(aes(x=System, y=frontier_distance_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Distance to Pareto-Front') +
    xlab('') +
    theme_classic() +
    coord_flip()
num_pfront

num_unif= num_systems %>% 
    ggplot(aes(x=System, y=rate_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Complexity') +
    xlab('') +
    theme_classic() +
    coord_flip()
num_unif

num_punif= num_psystems %>% 
    ggplot(aes(x=System, y=rate_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Complexity') +
    xlab('') +
    theme_classic() +
    coord_flip()
num_punif

num_div= num_systems %>% 
    ggplot(aes(x=System, y=distortion_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Information Loss') +
    xlab('') +
    theme_classic() +
    coord_flip()
num_div

num_pdiv= num_psystems %>% 
    ggplot(aes(x=System, y=distortion_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Information Loss') +
    xlab('') +
    theme_classic() +
    coord_flip()
num_pdiv

num_gNID= num_systems %>% 
    ggplot(aes(x=System, y=gNID, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Generalized Normalized Information Distance') +
    xlab('') +
    theme_classic() +
    coord_flip()
num_gNID

((num_unif + num_punif) / (num_div + num_pdiv) / (num_front + num_pfront) / num_gNID)

```

## Contiguous Permutations

```{r}


# Load data
num_detSys = read.csv('../Output/Number/permSys_cont.csv')
num_attested = read.csv('../Output/Number/attested.csv')

num_systems = num_attested %>% 
    select(rate_round, distortion_round, frontier_dist_round, gNID) %>% distinct() %>%
    full_join(num_detSys %>% 
                  mutate(rate_round=round(rate,4), distortion_round=round(distortion,4)) %>% 
                  select(X, rate_round, distortion_round, front=frontier_dist)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Possible', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round)) 

table(num_systems$System)

######## Formulas

# Complexity Formula
num_rec_rate <- 
  recipe(System ~ rate_round, data = num_systems) %>%
  step_rose(System)

# Information Loss Formula
num_rec_dist <- 
  recipe(System ~ distortion_round, data = num_systems) %>%
  step_rose(System)

# ParetoFront Formula
num_rec_front <- 
  recipe(System ~ frontier_distance_round, data = num_systems) %>%
  step_rose(System)

# gNID Formula
num_rec_gnid <- 
  recipe(System ~ gNID, data = num_systems) %>%
  step_rose(System)

######## Model

num_mod <-
  logistic_reg() %>%
  set_engine('glm')

# Workflow
num_rose_wflw <- 
  workflow() %>% 
  add_model(num_mod) 
num_rose_wflw

######## Evaluation

cv_folds <- vfold_cv(num_systems, strata = "System", repeats = 10)

logliklihood = function(...){ mn_log_loss(..., sum=TRUE)}
class(logliklihood) <- class(mn_log_loss)
attr(logliklihood, "direction") <- attr(mn_log_loss, "direction")

######## Fits

# Complexity

num_rose_res_rate <- fit_resamples(
  num_rose_wflw %>%
      add_recipe(num_rec_rate), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(num_rose_res_rate)

# Information Loss

num_rose_res_dist <- fit_resamples(
  num_rose_wflw %>%
      add_recipe(num_rec_dist), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(num_rose_res_dist)

# ParetoFront

num_rose_res_front <- fit_resamples(
  num_rose_wflw %>%
      add_recipe(num_rec_front), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(num_rose_res_front)


```

## Contiguous Deterministic

```{r}


# Load data
num_detSys = read.csv('../Output/Number/detSys_cont.csv')
num_attested = read.csv('../Output/Number/attested.csv')

num_systems = num_attested %>% 
    select(rate_round, distortion_round, frontier_dist_round, gNID) %>% distinct() %>%
    full_join(num_detSys %>% 
                  mutate(rate_round=round(rate,4), distortion_round=round(distortion,4)) %>% 
                  select(X, rate_round, distortion_round, front=frontier_dist)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Possible', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round)) 

table(num_systems$System)

######## Formulas

# Complexity Formula
num_rec_rate <- 
  recipe(System ~ rate_round, data = num_systems) %>%
  step_rose(System)

# Information Loss Formula
num_rec_dist <- 
  recipe(System ~ distortion_round, data = num_systems) %>%
  step_rose(System)

# ParetoFront Formula
num_rec_front <- 
  recipe(System ~ frontier_distance_round, data = num_systems) %>%
  step_rose(System)

# gNID Formula
num_rec_gnid <- 
  recipe(System ~ gNID, data = num_systems) %>%
  step_rose(System)

######## Model

num_mod <-
  logistic_reg() %>%
  set_engine('glm')

# Workflow
num_rose_wflw <- 
  workflow() %>% 
  add_model(num_mod) 
num_rose_wflw

######## Evaluation

cv_folds <- vfold_cv(num_systems, strata = "System", repeats = 10)

logliklihood = function(...){ mn_log_loss(..., sum=TRUE)}
class(logliklihood) <- class(mn_log_loss)
attr(logliklihood, "direction") <- attr(mn_log_loss, "direction")

######## Fits

# Complexity

num_rose_res_rate <- fit_resamples(
  num_rose_wflw %>%
      add_recipe(num_rec_rate), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(num_rose_res_rate)

# Information Loss

num_rose_res_dist <- fit_resamples(
  num_rose_wflw %>%
      add_recipe(num_rec_dist), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(num_rose_res_dist)

# ParetoFront

num_rose_res_front <- fit_resamples(
  num_rose_wflw %>%
      add_recipe(num_rec_front), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(num_rose_res_front)


```
