---
title: 'Tense Continuous Analysis'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/usr/bin/python3", required = TRUE)

library(tidyverse)
library(patchwork)
library(ggrepel)
library(ggridges)
library(tidyverse)
library(tidymodels)
library(themis)

set.seed(1234)
```

```{r plot_guides}
te_scale = scale_fill_manual(values=c('#be29ec', '#d896ff', '#efbbff', '#ae0001', '#008080', '#66b2b2', '#b2d8d8'))
ter_scale = scale_fill_manual(values=rev(c('#be29ec', '#d896ff', '#efbbff', '#ae0001', '#008080', '#66b2b2', '#b2d8d8')))
```

# Tense

Let's make some channel codes for existing languages.

```{python tense_channels}
import numpy as np
import pandas
from collections import namedtuple
from ibhelpers import *
from scipy.spatial import distance


datapath =  '../Data/Tense_Meanings.csv'

bd =  pandas.read_csv(datapath)


def present(x):
    x =  str(x)
    x = str.split(x, ';')[0]
    return  (x ==  'i' or x  == 'p')


def extract_cats(td):
    cats = []
    if present(td.past):
        cats.append([0,1,2])
    if present(td.present):
        cats.append([3])
    if present(td.future):
        cats.append([4,5,6])
    if present(td.immediate_past):
        cats.append([2])
    if present(td.recent_past):
        cats.append([1])
    if present(td.remote_past):
        cats.append([0])
    if present(td.immediate_future):
        cats.append([4])
    if present(td.remote_future):
        cats.append([5,6])
    if present(td.non_past):
        cats.append([3,4,5,6])
    if present(td.non_future):
        cats.append([0,1,2,3])
    if present(td.pre_hodiernal):
        cats.append([0,1])
    if present(td.past_wo_remote):
        cats.append([1,2])
    #if pandas.notna(td.bybee_alone) or pandas.notna(td.drop_language):
    #        cats = None
    return cats


ef = extract_cats
n = 7

bd['categories'] = bd.apply(ef,  axis=1)
bd = bd[['Language', 'Family', 'Source', 'categories']]

def unpackoutputs(row):
    # if multiple words for category assume uniform distribution for now -- revisit  later
    row['q'], row['lens']= cats2q(row['categories'], n)
    return row

bd = bd.apply(unpackoutputs, axis=1)
bd['ind'] = range(bd.shape[0])
```

Now let's specify the meanings P(xy) and the need probability P(x).

```{python tense_specification}

def domain_spec(n, det=0):
    domain = namedtuple('domain', ['n', 'items', 'p_xGy', 'q0', 'p_x', 'attested_path'])
    
    tinylam = 0.00000001
    eps = 0.01
    
    
    # probabilities from Geoff's manuscript
    allp =  np.array([0.1034, 0.0795, 0.1839, 0.6183, 0.0074, 0.0048, 0.0028])
    
    pastprops = allp[0:3]/np.sum(allp[0:3])
    futprops = allp[4:7]/np.sum(allp[4:7])
    duboisprobs = np.array([27.4, 47.5, 25.1]) # From Twitter Corpus
    duboisprobs = duboisprobs / np.sum(duboisprobs)
    allp = np.concatenate( [duboisprobs[0] * pastprops, duboisprobs[[1]], duboisprobs[2]*futprops ] )
    
    if n == 7:
        items = ['a', 'b', 'c', 'r', 'x', 'y', 'z']
        kap = 0.5
        lam = 0.1
        
        q0 = (1 - eps) * np.eye(7) + eps * np.ones((7, 7))
        p_xGy = np.array(
            [[1, kap, kap ** 2, lam * kap ** 2, lam ** 2 * kap ** 2, lam ** 2 * kap ** 3, lam ** 2 * kap ** 4],
             [kap, 1, kap, lam * kap, lam ** 2 * kap, lam ** 2 * kap ** 2, lam ** 2 * kap ** 3],
             [kap ** 2, kap, 1, lam, lam ** 2, lam ** 2 * kap, lam ** 2 * kap ** 2],
             [lam * kap ** 2, lam * kap, lam, 1, lam, lam * kap, lam * kap ** 2],
             [lam ** 2 * kap ** 2, lam ** 2 * kap, lam ** 2, lam, 1, kap, kap ** 2],
             [lam ** 2 * kap ** 3, lam ** 2 * kap ** 2, lam ** 2 * kap, lam * kap, kap, 1, kap],
             [lam ** 2 * kap ** 4, lam ** 2 * kap ** 3, lam ** 2 * kap ** 2, lam * kap ** 2, kap ** 2, kap, 1]])
        
        p_x= allp
        
        p_x = p_x / np.sum(p_x)
        
        attested_path = datapath
    else:
        raise Exception('unexpected n')
        
    d = domain(n, items, p_xGy, q0, p_x, attested_path)
    
    return d

detflag = 0   # 1 for deterministic speaker
n = 7 

d = domain_spec(n, detflag)

items = d.items
p_xGy = d.p_xGy
q0 = d.q0
p_x = d.p_x
attested_path = d.attested_path
p_mGs = p_xGy / np.sum(p_xGy, axis=0)

p_xGy = p_xGy / p_xGy.sum(axis=1, keepdims=True)
p_xy = p_xGy * p_x[:, np.newaxis]
p_xy = p_xy / np.sum(p_xy)

```

Plot the assumptions

```{r tense_fig1}

tense_meaning = py$p_xGy %>%
    as.data.frame() %>%
    rename(m_a = V1, m_b = V2, m_c = V3, m_r = V4, m_x = V5, m_y = V6, m_z = V7) %>%
    mutate(x=c('R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz')) %>%
    gather(goal, p, m_a:m_z) %>%
    mutate(goal = fct_relevel(goal, 'm_z', 'm_y', 'm_x', 'm_r', 'm_c', 'm_b', 'm_a'),
           x=fct_relevel(x, 'R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz'))

tense_meaning = py$p_xGy %>%
    as.data.frame() %>%
    rename(`s[a]` = V1, `s[b]` = V2, `s[c]` = V3, `s[r]` = V4, `s[x]` = V5, `s[y]` = V6, `s[z]` = V7) %>%
    mutate(x=c('R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz')) %>%
    gather(goal, p, `s[a]`:`s[z]`) %>%
    mutate(goal = fct_relevel(goal, 's[z]', 's[y]', 's[x]', 's[r]', 's[c]', 's[b]', 's[a]'),
           x=fct_relevel(x, 'R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz'))

tense_pxy = tense_meaning %>%
    ggplot(aes(x=x, y=goal, group=goal, height=p)) +
    geom_density_ridges(stat='identity', aes(fill=goal), 
                        alpha=0.8, scale=0.95) +
    theme_classic(base_size = 18) +
    guides(fill=F) +
    ggtitle('Tense') +
    ylab('Meaning') +
    xlab('') + 
    ter_scale +
    scale_y_discrete(labels=c(expression(m[z]), expression(m[y]), expression(m[x]), expression(m[r]), expression(m[c]), expression(m[b]), expression(m[a])))
tense_pxy

tense_pxy = tense_meaning %>%
    ggplot(aes(x=x, y=p, group=goal)) +
    facet_grid(goal~., scales = 'free_y', switch = 'y', as.table = F, labeller = label_parsed) +
    geom_hline(yintercept = 0) +
    geom_bar(stat='identity', aes(fill=goal), alpha=0.8) +
    theme_classic(base_size = 18) +
    guides(fill=F) +
    ggtitle('Tense') +
    ylab('Speaker Distribution') +
    xlab('World State') +
    ter_scale +
    theme(axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          axis.line.y = element_blank())
tense_pxy

tense_prior = data.frame(x=c('R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz'),
                         p=py$p_x) %>%
    mutate(x = fct_relevel(x, 'R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz'))

tense_px = tense_prior %>%
    ggplot(aes(x, p, fill=x)) +
    geom_bar(stat='identity') +
    theme_classic(base_size = 24) +
    ylab('Need Probability') +
    xlab('Meaning') +
    theme(plot.title = element_text(hjust = 0.5)) +
    coord_cartesian(ylim = c(0, 0.75)) +
    te_scale +
    scale_x_discrete(labels=c(expression(m[a]), expression(m[b]), expression(m[c]), expression(m[r]), expression(m[x]), expression(m[y]), expression(m[z]))) +
    guides(fill=F)
tense_px

tense_px = tense_prior %>%
    ggplot(aes(x, p, fill=x)) +
    geom_bar(stat='identity') +
    theme_classic(base_size = 18) +
    ylab('Prior Probability') +
    xlab('Speaker Distribution') +
    theme(plot.title = element_text(hjust = 0.5)) +
    coord_cartesian(ylim = c(0, 0.75)) +
    te_scale +
    scale_x_discrete(labels=c(expression(s[a]), expression(s[b]), expression(s[c]), expression(s[r]), expression(s[x]), expression(s[y]), expression(s[z]))) +
    guides(fill=F)
tense_px

tense_pxy / tense_px + plot_layout(heights = c(5, 1))

```

Let's score the attested systems

```{python tense_attested}
attested_data = bd
attested_data = attested_data.dropna()
attested_data['rate'] = 0
attested_data['distortion'] = 0
attested_data['exp_len'] = 0
attested_data['lab'] = ''
attested_perm_zc_scores= []
npitems = np.array(items)

for i in attested_data['ind']:
    q = attested_data.loc[i,'q']
    nat = naturalness(q, p_mGs)
    result = score_q_kl(p_xy, q)
    attested_data.loc[i,  'rate']  = result.rate
    attested_data.loc[i,  'distortion'] = result.distortion
    attested_data.loc[i,  'naturalness'] = np.sum(nat)
    lens = attested_data.loc[i, 'lens']
    attested_data.loc[i, 'exp_len'] = exp_len(q, p_x, lens)
    nc = len(attested_data.loc[i, 'categories'])
    attested_data.loc[i,  'n'] = nc
    attested_data.loc[i,  'lab'] = zmlabel(attested_data.loc[i, 'categories'], attested_data.loc[i,'lens'], npitems)
    attested_data.loc[i,  'zlab'] = zlabel(attested_data.loc[i, 'categories'], npitems)
    for j in np.arange(-1, nc):
        l  = make_lens(j, nc, n)
        e_len = exp_len(q, p_x, l)
        attested_perm_zc_scores.append( (result.rate, result.distortion, e_len))

```

Let's generate some possible systems

Permutations

```{python}
from itertools import permutations


def full_sorted(x):
    x = map(sorted, x)
    return sorted(x, key=lambda y: y[0])


def translate(system, perm):
    key = dict()
    for k, v in enumerate(perm):
        key[k] = v
    new_cats = []
    for cat in system:
        new_cat = []
        for o in cat:
            new_cat.append(key[o])
        new_cats.append(new_cat)
    return new_cats


def check_contig(key):
  tot = 0
  for categ in key:
    cat = list(categ)
    cat.sort()
    tally = 0
    for i, c in enumerate(cat):
      if c == cat[0]+i:
        tally += 1
    if tally == len(cat):
      tot += 1
  if tot == len(key):
    return True
  else:
    return False


perms = dict()
for p in permutations([0,1,2,3,4,5,6]):
    seen = set()
    for a in attested_data['categories']:
        if tuple(map(tuple, a)) not in seen:
            seen.add(tuple(map(tuple, a)))
            t = translate(a, p)
            if tuple(map(tuple, full_sorted(t))) not in perms.keys():
                perms[tuple(map(tuple, full_sorted(t)))] = t


ps_scores = []
ps_scores_explen = []
pqs = []
ps_names = []
ps_cats = []
for pk, ps in perms.items():
  if check_contig(pk):
    q = cats2q(ps, 7)[0]
    pqs.append(q)
    nat = naturalness(q, p_mGs)
    result = score_q_kl(p_xy, q)
    ps_scores.append( (result.rate, result.distortion, np.sum(nat)) )
    ps_names.append(pk)
    ps_cats.append(ps)
    nc = len(ps)
    for i in np.arange(-1, nc):
        l  = make_lens(i, nc, n)
        e_len = exp_len(q, p_x, l)
        ps_scores_explen.append( (result.rate, result.distortion, e_len))

```

Partitions

```{python tense_possible}
# deterministic systems with item labels
detsystems = list(partition(items))
# deterministic systems with item indices
iteminds=list(range(len(items)))
detsysteminds = list(partition(iteminds))

# score deterministic systems
ds_scores = []
ds_scores_explen = []
dqs = []
ds_cats = []
syslabs = []
for ds, dlab in zip(detsysteminds, detsystems):
  if check_contig(ds):
    q = partition2q(ds)
    dqs.append(q)
    nat = naturalness(q, p_mGs)
    result = score_q_kl(p_xy, q)
    ds_scores.append( (result.rate, result.distortion, np.sum(nat)))
    ds_cats.append(ds)
    syslabs.append(dlab)
    nc = len(ds)
    for i in np.arange(-1, nc):
        l  = make_lens(i, nc, n)
        e_len = exp_len(q, p_x, l)
        ds_scores_explen.append( (result.rate, result.distortion, e_len))
        

# make labels for deterministic systems
syslabels = [partition_label(p) for p in syslabs]

```

Calculate the frontier

```{python tense_ib}
# trace out optimal frontier
q0 = q0 / q0.sum(axis=1, keepdims=True)
betas = np.array([2.0 ** x for x in np.arange(5, 0, -0.001)])
focalbeta = 5.3

q, beta, ibscores, qresult, qseq, qseqresults, allqs = fit_ib(p_xy, q0, focalbeta, betas, verbose=1)
```

Calculate the distance metrics

```{python tense_postProcess}
# create data frames for plotting and analysis
ib_scores_df = pandas.DataFrame(np.array(ibscores), columns = ['rate', 'distortion', 'elen'])
ib_scores_df['beta'] = betas
ib_scores_df['q'] = allqs
ib_scores_df['Wn'] = [mergecols(q).shape[1] for q in ib_scores_df['q']]

# compute distance from optimal frontier
def fd(asys, ibscores):
    mind = distance.cdist([[asys['rate'], asys['distortion']]], ibscores[['rate', 'distortion']]).min()
    return mind


def gNID_d(asys, paretoQs, betas, pX):
    mind = np.zeros((len(asys), len(paretoQs)))
    for li in range(len(asys)):
        for qi, q in enumerate(paretoQs):
            mind[li, qi] = gNID(asys.iloc[li]['q'], q, pX)
    return np.argmin(mind, axis=1), np.min(mind, axis=1), betas[np.argmin(mind, axis=1)]


attested_data['frontier_dist'] = attested_data.apply(fd,  args = (ib_scores_df,), axis=1)
attested_data['rate_round'] = np.round(attested_data['rate'], 4)
attested_data['distortion_round'] = np.round(attested_data['distortion'], 4)
attested_data['frontier_dist_round'] = np.round(attested_data['frontier_dist'], 4)
attested_data['exp_len_round'] = np.round(attested_data['exp_len'], 4)
_, attested_data['gNID'], attested_data['gNID_beta']  = gNID_d(attested_data, allqs, betas, p_x)
attested_data['Wn'] = [mergecols(q).shape[1] for q in attested_data['q']]

ds_scores_df = pandas.DataFrame(np.array(ds_scores), columns = ['rate', 'distortion', 'naturalness'])
ds_scores_df['name'] = np.array(syslabels)
ds_scores_df['categories'] = np.array(ds_cats)
ds_scores_df['q'] = dqs
ds_scores_df['n'] = ds_scores_df['categories'].apply(lambda x: len(x))
ds_scores_df['frontier_dist'] = ds_scores_df.apply(fd,  args = (ib_scores_df,), axis=1)
_, ds_scores_df['gNID'], ds_scores_df['gNID_beta']  = gNID_d(ds_scores_df, allqs, betas, p_x)
ds_scores_df['Wn'] = [mergecols(q).shape[1] for q in ds_scores_df['q']]

ps_scores_df = pandas.DataFrame(np.array(ps_scores), columns = ['rate', 'distortion', 'naturalness'])
ps_scores_df['name'] = np.array(ps_names)
ps_scores_df['categories'] = np.array(ps_cats)
ps_scores_df['q'] = pqs
ps_scores_df['n'] = ps_scores_df['categories'].apply(lambda x: len(x))
ps_scores_df['rate_round'] = np.round(ps_scores_df['rate'], 4)
ps_scores_df['distortion_round'] = np.round(ps_scores_df['distortion'], 4)
ps_scores_df['frontier_dist'] = ps_scores_df.apply(fd,  args = (ib_scores_df,), axis=1)
_, ps_scores_df['gNID'], ps_scores_df['gNID_beta']  = gNID_d(ps_scores_df, allqs, betas, p_x)
ps_scores_df['Wn'] = [mergecols(q).shape[1] for q in ps_scores_df['q']]

ds_scores_explen_df = pandas.DataFrame(np.array(ds_scores_explen), columns = ['rate', 'distortion', 'exp_len'])
ps_scores_explen_df = pandas.DataFrame(np.array(ps_scores_explen), columns = ['rate', 'distortion', 'exp_len'])
attested_perm_zc_df = pandas.DataFrame(np.array(attested_perm_zc_scores), columns = ['rate', 'distortion', 'exp_len'])

ds_scores_df.to_csv('../Output/Tense/detSys_cont.csv')
ds_scores_explen_df.to_csv('../Output/Tense/detSysELen_cont.csv')
ps_scores_df.to_csv('../Output/Tense/permSys_cont.csv')
ps_scores_explen_df.to_csv('../Output/Tense/permSysELen_cont.csv')

```

Let's plot the pareto-frontier

```{r tense_fig2}
tense_pareto =  py$ib_scores_df
tense_detSys =  py$ds_scores_df
tense_detSysLen =  py$ds_scores_explen_df
tense_permSys = py$ps_scores_df
tense_attested =  py$attested_data
tense_attestedLen = py$attested_perm_zc_df

tense_ag = tense_attested %>%
    group_by(zlab) %>%
    summarise(rate=mean(rate), 
            distortion=mean(distortion),
            frontier_dist=first(frontier_dist),
            N=n(), 
            lab=first(zlab),
            instance=first(Language))

tense_ag2 = tense_attested %>%
    group_by(Language) %>%
    mutate(q = paste0(unlist(q), collapse='')) %>%
    ungroup() %>%
    group_by(q) %>%
    summarise(rate=mean(rate),
              distortion=mean(distortion),
              N=n(),
              Wn=first(Wn),
              lab=first(q))

tense_attested %>% pull(Language) %>% unique() %>% length()

tense_frontier = tense_pareto %>%
  ggplot(aes(rate, distortion)) +
  geom_area(fill='grey60') +
  geom_line() +
  geom_point(data=tense_detSys, color='grey80') +
  geom_point(data=tense_permSys, color='darkorchid1', alpha=0.3) +
  geom_point(data=tense_ag, aes(size=N)) +
  #geom_text_repel(data=tense_ag %>% filter(N > 10), aes(label=lab)) +
  ylab('Information Loss') +
  xlab('Complexity') +
  theme_classic(base_size = 14) +
  ggtitle('Tense') +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_cartesian(ylim=c(0, 1), xlim=c(0, 2))
tense_frontier

tense_frontier_simp = tense_detSys %>%
  select(Wn, distortion) %>%
  distinct() %>%
  ggplot(aes(Wn, distortion)) +
  geom_point(color='grey80') +
  geom_point(data=tense_permSys, color='darkorchid1', alpha=0.3) +
  geom_point(data=tense_ag2, aes(size=N)) +
  ylab('Information Loss') +
  xlab('Inventory Complexity') +
  theme_classic(base_size = 14) +
  ggtitle('Tense') +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks=1:7)
tense_frontier_simp

tense_systems = tense_attested %>% 
    select(rate_round, distortion_round, frontier_dist_round, gNID_beta, gNID) %>% distinct() %>%
    full_join(tense_detSys %>% select(name, rate_round=rate, distortion_round=distortion, front=frontier_dist, gNID_beta, gNID)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Possible', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round))

tense_psystems = tense_attested %>% 
    select(rate_round, distortion_round, frontier_dist_round, gNID_beta, gNID) %>% distinct() %>%
    full_join(tense_permSys %>% select(name, rate_round=rate, distortion_round=distortion, front=frontier_dist, gNID_beta, gNID)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Permuted', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round))

tense_front = tense_systems %>%
    ggplot(aes(x=System, y=frontier_distance_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Distance to Pareto-Front') +
    xlab('') +
    theme_classic() +
    coord_flip()
tense_front

tense_pfront = tense_psystems %>%
    ggplot(aes(x=System, y=frontier_distance_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Distance to Pareto-Front') +
    xlab('') +
    theme_classic() +
    coord_flip()
tense_pfront

tense_gNID = tense_systems %>%
    ggplot(aes(x=System, y=gNID, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Generalized Normalized Information Distance') +
    xlab('') +
    theme_classic() +
    coord_flip()
tense_gNID

tense_pgNID = tense_psystems %>%
    ggplot(aes(x=System, y=gNID, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Generalized Normalized Information Distance') +
    xlab('') +
    theme_classic() +
    coord_flip()
tense_pgNID

tense_unif= tense_systems %>% 
    ggplot(aes(x=System, y=rate_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Complexity') +
    xlab('') +
    theme_classic() +
    coord_flip()
tense_unif

tense_punif= tense_psystems %>% 
    ggplot(aes(x=System, y=rate_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Complexity') +
    xlab('') +
    theme_classic() +
    coord_flip()
tense_punif

tense_div= tense_systems %>% 
    ggplot(aes(x=System, y=distortion_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Information Loss') +
    xlab('') +
    theme_classic() +
    coord_flip()
tense_div

tense_pdiv= tense_psystems %>% 
    ggplot(aes(x=System, y=distortion_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Information Loss') +
    xlab('') +
    theme_classic() +
    coord_flip()
tense_pdiv

((tense_unif + tense_punif) / (tense_div + tense_pdiv) / (tense_front + tense_pfront) / (tense_gNID + tense_pgNID))
```

## Deterministic

```{r}

# Load data
ten_detSys = read.csv('../Output/Tense/detSys_cont.csv')
ten_attested = read.csv('../Output/Tense/attested.csv')

ten_systems = ten_attested %>% 
    select(rate_round, distortion_round, frontier_dist_round, gNID) %>% distinct() %>%
    full_join(ten_detSys %>% 
                  mutate(rate_round=round(rate,4), distortion_round=round(distortion,4)) %>% 
                  select(X, rate_round, distortion_round, front=frontier_dist, gNID)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Permuted', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round)) 

table(ten_systems$System)

######## Formulas

# Complexity Formula
ten_rec_rate <- 
  recipe(System ~ rate_round, data = ten_systems) %>%
  step_rose(System)

# Information Loss Formula
ten_rec_dist <- 
  recipe(System ~ distortion_round, data = ten_systems) %>%
  step_rose(System)

# ParetoFront Formula
ten_rec_front <- 
  recipe(System ~ frontier_distance_round, data = ten_systems) %>%
  step_rose(System)

# gNID
ten_rec_gnid <- 
  recipe(System ~ gNID, data = ten_systems) %>%
  step_rose(System)

######## Model

ten_mod <-
  logistic_reg() %>%
  set_engine('glm')

# Workflow
ten_rose_wflw <- 
  workflow() %>% 
  add_model(ten_mod) 

######## Evaluation

cv_folds <- vfold_cv(ten_systems, strata = "System", repeats = 10)

logliklihood = function(...){ mn_log_loss(..., sum=TRUE)}
class(logliklihood) <- class(mn_log_loss)
attr(logliklihood, "direction") <- attr(mn_log_loss, "direction")

######## Fits

# Complexity
ten_rose_res_rate <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_rate), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_rate)

# Information Loss

ten_rose_res_dist <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_dist), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_dist)

# ParetoFront

ten_rose_res_front <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_front), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_front)

# gNID

ten_rose_res_gnid <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_gnid), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_gnid)

```

## Permuted

```{r}

# Load data
ten_detSys = read.csv('../Output/Tense/permSys_cont.csv')
ten_attested = read.csv('../Output/Tense/attested.csv')

ten_systems = ten_attested %>% 
    select(rate_round, distortion_round, frontier_dist_round, gNID) %>% distinct() %>%
    full_join(ten_detSys %>% 
                  mutate(rate_round=round(rate,4), distortion_round=round(distortion,4)) %>% 
                  select(X, rate_round, distortion_round, front=frontier_dist, gNID)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Permuted', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round)) 

table(ten_systems$System)

######## Formulas

# Complexity Formula
ten_rec_rate <- 
  recipe(System ~ rate_round, data = ten_systems) %>%
  step_rose(System)

# Information Loss Formula
ten_rec_dist <- 
  recipe(System ~ distortion_round, data = ten_systems) %>%
  step_rose(System)

# ParetoFront Formula
ten_rec_front <- 
  recipe(System ~ frontier_distance_round, data = ten_systems) %>%
  step_rose(System)

# gNID
ten_rec_gnid <- 
  recipe(System ~ gNID, data = ten_systems) %>%
  step_rose(System)

######## Model

ten_mod <-
  logistic_reg() %>%
  set_engine('glm')

# Workflow
ten_rose_wflw <- 
  workflow() %>% 
  add_model(ten_mod) 

######## Evaluation

cv_folds <- vfold_cv(ten_systems, strata = "System", repeats = 10)

logliklihood = function(...){ mn_log_loss(..., sum=TRUE)}
class(logliklihood) <- class(mn_log_loss)
attr(logliklihood, "direction") <- attr(mn_log_loss, "direction")

######## Fits

# Complexity
ten_rose_res_rate <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_rate), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_rate)

# Information Loss

ten_rose_res_dist <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_dist), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_dist)

# ParetoFront

ten_rose_res_front <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_front), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_front)

# gNID

ten_rose_res_gnid <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_gnid), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_gnid)

```
