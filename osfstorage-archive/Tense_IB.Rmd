---
title: "Tense IB"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lme4)
library(tidymodels)
library(themis)
library(patchwork)
library(ggrepel)
library(ggridges)
library(here)

# Just some color scales for visualization consistency
te_scale = scale_fill_manual(values=c('#be29ec', '#d896ff', '#efbbff', '#ae0001', '#008080', '#66b2b2', '#b2d8d8'))
ter_scale = scale_fill_manual(values=rev(c('#be29ec', '#d896ff', '#efbbff', '#ae0001', '#008080', '#66b2b2', '#b2d8d8')))

library(reticulate)
use_python("/usr/bin/python3", required = TRUE)
```

```{python config, echo=F}
import sys
sys.path.append(r.here())
import numpy as np
import pandas
from collections import namedtuple
from ibhelpers import *
from scipy.spatial import distance
```

# Tense

## Meaning

In this analysis, we will use the information bottleneck method to define an optimal frontier, score attested languages, score possible languages and check whether distance to the pareto-frontier is a better predictor of whether a language is attested than either complexity or information loss alone.

### Assumptions

The first thing we do is formalize our assumptions.

```{python tense_specification}

items = ['a', 'b', 'c', 'r', 'x', 'y', 'z']

# probabilities from Geoff's dissertation from Google N-grams (1985)
allp =  np.array([0.1034, 0.0795, 0.1839, 0.6183, 0.0074, 0.0048, 0.0028])
pastprops = allp[0:3]/np.sum(allp[0:3])
futprops = allp[4:7]/np.sum(allp[4:7])
duboisprobs = np.array([27.4, 47.5, 25.1]) # From Twitter Corpus
duboisprobs = duboisprobs / np.sum(duboisprobs)
p_x = np.concatenate( [duboisprobs[0] * pastprops, duboisprobs[[1]], duboisprobs[2]*futprops ] )
p_x = p_x / np.sum(p_x)

eps = 0.01
q0 = (1 - eps) * np.eye(7) + eps * np.ones((7, 7))

kap = 0.5
lam = 0.1

p_xGy = np.array(
    [[1, kap, kap ** 2, lam * kap ** 2, lam ** 2 * kap ** 2, lam ** 2 * kap ** 3, lam ** 2 * kap ** 4],
     [kap, 1, kap, lam * kap, lam ** 2 * kap, lam ** 2 * kap ** 2, lam ** 2 * kap ** 3],
     [kap ** 2, kap, 1, lam, lam ** 2, lam ** 2 * kap, lam ** 2 * kap ** 2],
     [lam * kap ** 2, lam * kap, lam, 1, lam, lam * kap, lam * kap ** 2],
     [lam ** 2 * kap ** 2, lam ** 2 * kap, lam ** 2, lam, 1, kap, kap ** 2],
     [lam ** 2 * kap ** 3, lam ** 2 * kap ** 2, lam ** 2 * kap, lam * kap, kap, 1, kap],
     [lam ** 2 * kap ** 4, lam ** 2 * kap ** 3, lam ** 2 * kap ** 2, lam * kap ** 2, kap ** 2, kap, 1]])

p_mGs = p_xGy / np.sum(p_xGy, axis=0)
p_xGy = p_xGy / p_xGy.sum(axis=1, keepdims=True)
p_xy = p_xGy * p_x[:, np.newaxis]
p_xy = p_xy / np.sum(p_xy)

```

Let's plot these assumptions.

```{r tense_fig1}

tense_prior = data.frame(x=c('R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz'),
                         p=py$p_x) %>%
    mutate(x = fct_relevel(x, 'R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz'))

tense_px = tense_prior %>%
    ggplot(aes(x, p, fill=x)) +
    geom_bar(stat='identity') +
    theme_classic(base_size = 18) +
    ylab('Prior Probability') +
    xlab('Speaker Distribution') +
    theme(plot.title = element_text(hjust = 0.5)) +
    coord_cartesian(ylim = c(0, 0.75)) +
    te_scale +
    scale_x_discrete(labels=c(expression(s[a]), expression(s[b]), expression(s[c]), expression(s[r]), expression(s[x]), expression(s[y]), expression(s[z]))) +
    guides(fill=F)
tense_px

tense_meaning = py$p_xGy %>%
    as.data.frame() %>%
    rename(`s[a]` = V1, `s[b]` = V2, `s[c]` = V3, `s[r]` = V4, `s[x]` = V5, `s[y]` = V6, `s[z]` = V7) %>%
    mutate(x=c('R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz')) %>%
    gather(goal, p, `s[a]`:`s[z]`) %>%
    mutate(goal = fct_relevel(goal, 's[z]', 's[y]', 's[x]', 's[r]', 's[c]', 's[b]', 's[a]'),
           x=fct_relevel(x, 'R Past\na', 'Past\nb', 'N Past\nc', 'Present\nr', 'N Future\nx', 'Future\ny', 'R Future\nz'))

tense_pxy = tense_meaning %>%
    ggplot(aes(x=x, y=p, group=goal)) +
    facet_grid(goal~., scales = 'free_y', switch = 'y', as.table = F, labeller = label_parsed) +
    geom_hline(yintercept = 0) +
    geom_bar(stat='identity', aes(fill=goal), alpha=0.8) +
    theme_classic(base_size = 18) +
    guides(fill=F) +
    ggtitle('Tense') +
    ylab('Speaker Distribution') +
    xlab('World State') +
    ter_scale +
    theme(axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          axis.line.y = element_blank())
tense_pxy

```

### The Pareto Frontier

Now let's run the Information Bottleneck Method

```{python tense_ib}
# trace out optimal frontier
q0 = q0 / q0.sum(axis=1, keepdims=True)
betas = np.array([2.0 ** x for x in np.arange(5, 0, -0.001)])
focalbeta = 5.3

q, beta, ibscores, qresult, qseq, qseqresults, allqs = fit_ib(p_xy, q0, focalbeta, betas, verbose=1)

# create data frames for plotting and analysis
ib_scores_df = pandas.DataFrame(np.array(ibscores), columns = ['rate', 'distortion', 'elen'])
ib_scores_df['beta'] = betas
ib_scores_df['q'] = allqs
ib_scores_df['Wn'] = [mergecols(q).shape[1] for q in ib_scores_df['q']]

# The structural phase transitions along the pareto frontier
stochSys = []
for i, q in enumerate(zip(qseq, qseqresults)):
    for w in mergecols(q[0]).transpose():
        stochSys.append([len(qseq)-i, q[1][0], q[1][1]] + list(w))

stochSys = pandas.DataFrame(data=np.array(stochSys), columns = ['n', 'rate', 'distortion'] + items)

```

```{python frontier_metrics}

# compute distance from optimal frontier
def fd(asys, ibscores):
    mind = distance.cdist([[asys['rate'], asys['distortion']]], ibscores[['rate', 'distortion']]).min()
    return mind


def gNID_d(asys, paretoQs, betas, pX):
    mind = np.zeros((len(asys), len(paretoQs)))
    for li in range(len(asys)):
        for qi, q in enumerate(paretoQs):
            mind[li, qi] = gNID(asys.iloc[li]['q'], q, pX)
    return np.argmin(mind, axis=1), np.min(mind, axis=1), betas[np.argmin(mind, axis=1)]


```

### Attested Languages

First we load and pre-process

```{python tense_channels}

bd =  pandas.read_csv('Data/Tense_Meanings.csv')


def present(x):
    '''
      Is this distinction present in a language?
    '''
    x =  str(x)
    x = str.split(x, ';')[0]
    return  (x ==  'i' or x  == 'p')


def extract_cats(td):
    '''
      Map a present category into an interval of the domain?
    '''
    cats = []
    if present(td.past):
        cats.append([0,1,2])
    if present(td.present):
        cats.append([3])
    if present(td.future):
        cats.append([4,5,6])
    if present(td.immediate_past):
        cats.append([2])
    if present(td.recent_past):
        cats.append([1])
    if present(td.remote_past):
        cats.append([0])
    if present(td.immediate_future):
        cats.append([4])
    if present(td.remote_future):
        cats.append([5,6])
    if present(td.non_past):
        cats.append([3,4,5,6])
    if present(td.non_future):
        cats.append([0,1,2,3])
    if present(td.pre_hodiernal):
        cats.append([0,1])
    if present(td.past_wo_remote):
        cats.append([1,2])
    return cats


ef = extract_cats
n = 7

bd['categories'] = bd.apply(ef,  axis=1)
bd = bd[['Language', 'Family', 'Source', 'categories']]


def unpackoutputs(row):
    # if multiple words for category assume uniform distribution for now -- revisit  later
    row['q'], row['lens']= cats2q(row['categories'], n)
    return row

bd = bd.apply(unpackoutputs, axis=1)
bd['ind'] = range(bd.shape[0])
```

Then we score.

```{python tense_attested}
attested_data = bd
attested_data = attested_data.dropna()
attested_data['rate'] = 0
attested_data['distortion'] = 0
attested_data['exp_len'] = 0
attested_data['lab'] = ''
attested_perm_zc_scores= []
npitems = np.array(items)

for i in attested_data['ind']:
    q = attested_data.loc[i,'q']
    nat = naturalness(q, p_mGs)
    result = score_q_kl(p_xy, q)
    attested_data.loc[i,  'rate']  = result.rate
    attested_data.loc[i,  'distortion'] = result.distortion
    attested_data.loc[i,  'naturalness'] = np.sum(nat)
    lens = attested_data.loc[i, 'lens']
    attested_data.loc[i, 'exp_len'] = exp_len(q, p_x, lens)
    nc = len(attested_data.loc[i, 'categories'])
    attested_data.loc[i,  'n'] = nc
    attested_data.loc[i,  'lab'] = zmlabel(attested_data.loc[i, 'categories'], attested_data.loc[i,'lens'], npitems)
    attested_data.loc[i,  'zlab'] = zlabel(attested_data.loc[i, 'categories'], npitems)
    for j in np.arange(-1, nc):
        l  = make_lens(j, nc, n)
        e_len = exp_len(q, p_x, l)
        attested_perm_zc_scores.append( (result.rate, result.distortion, e_len))

attested_data['frontier_dist'] = attested_data.apply(fd,  args = (ib_scores_df,), axis=1)
attested_data['rate_round'] = np.round(attested_data['rate'], 4)
attested_data['distortion_round'] = np.round(attested_data['distortion'], 4)
attested_data['frontier_dist_round'] = np.round(attested_data['frontier_dist'], 4)
attested_data['exp_len_round'] = np.round(attested_data['exp_len'], 4)
_, attested_data['gNID'], attested_data['gNID_beta']  = gNID_d(attested_data, allqs, betas, p_x)
attested_data['Wn'] = [mergecols(q).shape[1] for q in attested_data['q']]

attested_perm_zc_df = pandas.DataFrame(np.array(attested_perm_zc_scores), columns = ['rate', 'distortion', 'exp_len'])

# Let's do a quick reformat for plotting
channels = []
for i in attested_data['ind']:
    q = attested_data.loc[i, 'q']
    for w in mergecols(q).transpose():
        channels.append([attested_data.loc[i, 'Language'], attested_data.loc[i, 'rate'], attested_data.loc[i, 'distortion'], attested_data.loc[i, 'gNID_beta']] + list(w))

channels = pandas.DataFrame(data=np.array(channels), columns = ['Language', 'rate', 'distortion', 'beta'] + items)

```

### Possible Systems

Let's generate and score some possible systems

```{python tense_possible}
# deterministic systems with item labels
detsystems = list(partition(items))
# deterministic systems with item indices
iteminds=list(range(len(items)))
detsysteminds = list(partition(iteminds))

# score deterministic systems
ds_scores = []
ds_scores_explen = []
dqs = []
for ds in detsysteminds:
    q = partition2q(ds)
    dqs.append(q)
    nat = naturalness(q, p_mGs)
    result = score_q_kl(p_xy, q)
    ds_scores.append( (result.rate, result.distortion, np.sum(nat)))
    nc = len(ds)
    for i in np.arange(-1, nc):
        l  = make_lens(i, nc, n)
        e_len = exp_len(q, p_x, l)
        ds_scores_explen.append( (result.rate, result.distortion, e_len))
        

# make labels for deterministic systems
syslabels = [partition_label(p) for p in detsystems]

ds_scores_df = pandas.DataFrame(np.array(ds_scores), columns = ['rate', 'distortion', 'naturalness'])
ds_scores_df['name'] = np.array(syslabels)
ds_scores_df['categories'] = np.array(detsysteminds)
ds_scores_df['q'] = dqs
ds_scores_df['n'] = ds_scores_df['categories'].apply(lambda x: len(x))
ds_scores_df['frontier_dist'] = ds_scores_df.apply(fd,  args = (ib_scores_df,), axis=1)
_, ds_scores_df['gNID'], ds_scores_df['gNID_beta']  = gNID_d(ds_scores_df, allqs, betas, p_x)
ds_scores_df['Wn'] = [mergecols(q).shape[1] for q in ds_scores_df['q']]

ds_scores_explen_df = pandas.DataFrame(np.array(ds_scores_explen), columns = ['rate', 'distortion', 'exp_len'])

```

```{python tense_postProcess, echo=F, eval=F}
# Save the partial outputs

ib_scores_df.to_csv('Output/Tense/pareto.csv')
ds_scores_df.to_csv('Output/Tense/detSys.csv')
ds_scores_explen_df.to_csv('Output/Tense/detSysELen.csv')
attested_data.to_csv('Output/Tense/attested.csv')
attested_perm_zc_df.to_csv('Output/Tense/attestedLen.csv')
stochSys.to_csv('Output/Tense/stochSys.csv')
channels.to_csv('Output/Tense/channels.csv')

```

### Putting it all Together

```{r tense_fig2}
tense_pareto = py$ib_scores_df
tense_detSys = py$ds_scores_df
tense_detSysLen = py$ds_scores_explen_df
tense_attested = py$attested_data
tense_attestedLen = py$attested_perm_zc_df

tense_ag = tense_attested %>%
    group_by(zlab) %>%
    summarise(rate=mean(rate), 
            distortion=mean(distortion),
            frontier_dist=first(frontier_dist),
            N=n(), 
            lab=first(zlab),
            instance=first(Language))

tense_ag2 = tense_attested %>%
    group_by(Language) %>%
    mutate(q = paste0(unlist(q), collapse='')) %>%
    ungroup() %>%
    group_by(q) %>%
    summarise(rate=mean(rate),
              distortion=mean(distortion),
              N=n(),
              Wn=first(Wn),
              lab=first(q))

# Double Check Number of Languages
tense_attested %>% pull(Language) %>% unique() %>% length()

tense_frontier = tense_pareto %>%
  ggplot(aes(rate, distortion)) +
  geom_area(fill='grey60') +
  geom_line() +
  geom_point(data=tense_detSys, color='grey80') +
  geom_point(data=tense_ag, aes(size=N)) +
  #geom_text_repel(data=tense_ag %>% filter(N > 10), aes(label=lab)) +
  ylab('Information Loss') +
  xlab('Complexity') +
  theme_classic(base_size = 14) +
  ggtitle('Tense') +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_cartesian(ylim=c(0, 1), xlim=c(0, 2))
tense_frontier

tense_frontier_simp = tense_detSys %>%
  select(Wn, distortion) %>%
  distinct() %>%
  ggplot(aes(Wn, distortion)) +
  geom_point(color='grey80') +
  geom_point(data=tense_ag2, aes(size=N)) +
  ylab('Information Loss') +
  xlab('Inventory Complexity') +
  theme_classic(base_size = 14) +
  ggtitle('Tense') +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks=1:7)
tense_frontier_simp

```

### Attested vs Possible Prediction

```{r}

tense_systems = tense_attested %>% 
    select(rate_round, distortion_round, frontier_dist_round, gNID_beta, gNID) %>% distinct() %>%
    full_join(tense_detSys %>% select(name, rate_round=rate, distortion_round=distortion, front=frontier_dist, gNID_beta, gNID)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Possible', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round))

tense_front = tense_systems %>%
    ggplot(aes(x=System, y=frontier_distance_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Distance to Pareto-Front') +
    xlab('') +
    theme_classic() +
    coord_flip()

tense_gNID = tense_systems %>%
    ggplot(aes(x=System, y=gNID, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Generalized Normalized Information Distance') +
    xlab('') +
    theme_classic() +
    coord_flip()

tense_unif= tense_systems %>% 
    ggplot(aes(x=System, y=rate_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Complexity') +
    xlab('') +
    theme_classic() +
    coord_flip()

tense_div= tense_systems %>% 
    ggplot(aes(x=System, y=distortion_round, fill=System)) +
    geom_boxplot() +
    guides(fill=F) +
    ylab('Information Loss') +
    xlab('') +
    theme_classic() +
    coord_flip()

(tense_unif / tense_div / tense_front / tense_gNID)
```

```{r}
ten_systems = tense_attested %>% 
    select(rate_round, distortion_round, frontier_dist_round, gNID) %>% distinct() %>%
    full_join(tense_detSys %>% 
                  mutate(rate_round=round(rate,4), distortion_round=round(distortion,4)) %>% 
                  select(X, rate_round, distortion_round, front=frontier_dist, gNID)) %>%
    mutate(System = ifelse(is.na(frontier_dist_round), 'Possible', 'Attested'),
           frontier_distance_round = ifelse(is.na(frontier_dist_round), round(front, 4), frontier_dist_round)) 

table(ten_systems$System)

######## Formulas

# Complexity Formula
ten_rec_rate <- 
  recipe(System ~ rate_round, data = ten_systems) %>%
  step_rose(System)

# Information Loss Formula
ten_rec_dist <- 
  recipe(System ~ distortion_round, data = ten_systems) %>%
  step_rose(System)

# ParetoFront Formula
ten_rec_front <- 
  recipe(System ~ frontier_distance_round, data = ten_systems) %>%
  step_rose(System)

# gNID
ten_rec_gnid <- 
  recipe(System ~ gNID, data = ten_systems) %>%
  step_rose(System)

######## Model

ten_mod <-
  logistic_reg() %>%
  set_engine('glm')

# Workflow
ten_rose_wflw <- 
  workflow() %>% 
  add_model(ten_mod) 

######## Evaluation

cv_folds <- vfold_cv(ten_systems, strata = "System", repeats = 10)

logliklihood = function(...){ mn_log_loss(..., sum=TRUE)}
class(logliklihood) <- class(mn_log_loss)
attr(logliklihood, "direction") <- attr(mn_log_loss, "direction")

######## Fits

# Complexity
ten_rose_res_rate <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_rate), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_rate)

# Information Loss

ten_rose_res_dist <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_dist), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_dist)

# ParetoFront

ten_rose_res_front <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_front), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_front)

# gNID

ten_rose_res_gnid <- fit_resamples(
  ten_rose_wflw %>%
      add_recipe(ten_rec_gnid), 
  resamples = cv_folds,
  metrics = metric_set(accuracy, logliklihood),
  control = control_resamples(save_pred = TRUE)
)
collect_metrics(ten_rose_res_gnid)

######## Log likelihoods

ten_rose_res_rate %>%
  collect_predictions() %>%
  mutate(logLike = ifelse(System=='Attested', log(.pred_Attested), log(.pred_Possible))) %>%
  pull(logLike) %>% sum

ten_rose_res_dist %>%
  collect_predictions() %>%
  mutate(logLike = ifelse(System=='Attested', log(.pred_Attested), log(.pred_Possible))) %>%
  pull(logLike) %>% sum

ten_rose_res_front %>%
  collect_predictions() %>%
  mutate(logLike = ifelse(System=='Attested', log(.pred_Attested), log(.pred_Possible))) %>%
  pull(logLike) %>% sum

ten_rose_res_gnid %>%
  collect_predictions() %>%
  mutate(logLike = ifelse(System=='Attested', log(.pred_Attested), log(.pred_Possible))) %>%
  pull(logLike) %>% sum(na.rm=T)

```

## Form

### Overall correlation

```{r tense_forms}

dt = read.csv('Data/Tense_Forms.csv')

dti = tense_attested %>%
    select(Language, Family, Distortion=distortion)

attested = dt %>%
    mutate(Form = as.character(Form),
           Length = nchar(Form)) %>%
    group_by(Language, Meaning) %>%
    summarise(Length = mean(Length)) %>%
    ungroup() %>%
    mutate(p = 0,
           p = ifelse(grepl('a', Meaning), p+0.1034, p),
           p = ifelse(grepl('b', Meaning), p+0.0795, p),
           p = ifelse(grepl('c', Meaning), p+0.1839, p),
           p = ifelse(grepl('r', Meaning), p+0.6183, p),
           p = ifelse(grepl('x', Meaning), p+0.0074, p),
           p = ifelse(grepl('y', Meaning), p+0.0048, p),
           p = ifelse(grepl('z', Meaning), p+0.0028, p),
           len = ifelse(p==0, 0, -log2(p))
    ) %>%
    group_by(Language) %>%
    mutate(Length = Length / max(Length),
           len = len / max(len)) %>%
    ungroup() %>%
    left_join(dti)

tense_form = attested %>%
    group_by(Meaning) %>%
    summarise(LengthU = mean(Length) + sd(Length) / sqrt(n()),
              LengthL = mean(Length) - sd(Length)  / sqrt(n()),
              Length = mean(Length),
              lenU = mean(len) + sd(len)  / sqrt(n()),
              lenL = mean(len) - sd(len)  / sqrt(n()),
              len = mean(len)
    ) %>%
    ungroup() %>%
    ggplot(aes(Length, len, label=Meaning, color=Meaning)) +
    stat_smooth(aes(group=Language), method=lm, se=F, color='grey90', alpha=0.01, data=attested) +
    geom_segment(aes(x=LengthL, xend=LengthU, y=len, yend=len)) +
    geom_segment(aes(x=Length, xend=Length, y=lenL, yend=lenU)) +
    geom_text_repel(size=8) +
    ylab('Optimal Length') +
    xlab('Observed Length') +
    ggtitle('Tense') +
    guides(color=F) +
    theme_classic(base_size = 14)
tense_form

fit_t1 = lmer(Length ~ len + (1|Family) + (1|Language), data=attested)
summary(fit_t1)

fit_t0 = lmer(Length ~ 1 + (1|Family) + (1|Language), data=attested)
summary(fit_t0)

anova(fit_t1, fit_t0)

```

### Individual Languages

```{r}

att_corr = attested %>%
  group_by(Language) %>%
  summarise(correlation = cor(Length, len)) %>%
  ungroup()

tense_corr = attested %>%
  left_join(att_corr) %>%
  mutate(Language = fct_reorder(Language, -correlation)) %>%
  ggplot(aes(Length, len)) +
  facet_wrap(~Language, ncol = 7, as.table = T) +
  stat_smooth(method=lm, se=F,  color='grey70', alpha=0.01) +
  geom_text_repel(aes(label=Meaning)) +
  geom_label(aes(x=0.1, y=1, label=round(correlation, 2))) +  
  ggtitle('Tense') +
  ylab('Optimal Length') +
  xlab('Observed Length') +
  theme_classic(base_size = 10) +
  theme(aspect.ratio = 1)
tense_corr

```

### Zero Marking Analysis

```{r, tense_0mark}

tense_len = anti_join(
  tense_detSysLen %>% 
    mutate(rate=round(rate, 4),
           distortion=round(distortion, 4),
           exp_len = round(exp_len, 4)), 
  tense_attested %>% 
    select(rate, distortion, exp_len) %>%
    mutate(rate=round(rate, 4),
           distortion=round(distortion, 4),
           exp_len = round(distortion, 4))
  ) %>%
  anti_join(tense_attestedLen %>% 
    mutate(rate=round(rate, 4),
           distortion=round(distortion, 4),
           exp_len = round(exp_len, 4)))

tense_length = tense_len %>% 
  group_by(rate, distortion, exp_len) %>%
  summarise(N=n()) %>%
  ungroup() %>%
  mutate(Model='Possible') %>%
  ggplot(aes(exp_len, distortion, color=Model)) +
  geom_point() +
  geom_point(data = tense_attestedLen %>%
               group_by(rate, distortion, exp_len) %>%
               summarise(N=n()) %>%
               ungroup() %>%
               mutate(Model='Permuted')) +
  geom_point(data = tense_attested %>%
               filter(Language != 'Acoma') %>% # Acoma's lens is wrong
               group_by(rate, distortion, exp_len) %>%
               summarise(N=n()) %>%
               ungroup() %>%
               mutate(Model='Attested'),
             aes(size=N)) +
  scale_color_manual(values=c('black', 'cornflowerblue', 'grey80')) +
  xlab('Expected Length') +
  ylab('Information Loss') +
  guides(color=F) +
  theme_classic(base_size = 14) +
  theme(legend.position = c(.1, .2))
tense_length

ten_base = tense_attested %>%
  filter(distortion < max(distortion)) %>%
  mutate(ZM = ifelse(exp_len == 1, 0, 1)) %>%
  glmer(ZM ~ 1 + (1|Family), family=binomial(link='logit'), data=.)
summary(ten_base)

ten_zm = tense_attested %>%
  filter(distortion < max(distortion)) %>%
  mutate(ZM = ifelse(exp_len == 1, 0, 1)) %>%
  glmer(ZM ~ distortion + (1|Family), family=binomial(link='logit'), data=.)
summary(ten_zm)

anova(ten_base, ten_zm)

```

