{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Communication Efficiency and Learnability of Colors using an Information Bottleneck Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ibhelpers import *\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from the World Color Survey\n",
    "df = pd.read_csv(\"wcs/term.txt\", delimiter=\"\\t\", header=None)\n",
    "df.columns = [\"language\", \"speaker\", \"chip\", \"word\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate prior probabilities\n",
    "\n",
    "For each language, $l$, we can use a frequentist approach (counts) to calculate the observed quantities of $p(w|c,l)$ and $p(c|w,l)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language  chip  word\n",
       "1         1     F       0.08\n",
       "                G       0.52\n",
       "                LB      0.36\n",
       "                LF      0.04\n",
       "          2     F       0.60\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_chip_count_df = df.groupby([\"language\", \"chip\", \"word\"]).speaker.agg(individual_count_per_word_per_chip=\"count\")\n",
    "total_word_count_df = df.groupby([\"language\", \"chip\"]).word.agg(total_words_per_chip=\"count\")\n",
    "\n",
    "# frequentist probability of a word given chip and language\n",
    "p_word_chip_language = per_chip_count_df[\"individual_count_per_word_per_chip\"] / total_word_count_df[\"total_words_per_chip\"]\n",
    "p_word_chip_language.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language  word  chip\n",
       "1         F     1       0.001366\n",
       "                2       0.010246\n",
       "                4       0.001366\n",
       "                5       0.011612\n",
       "                6       0.003415\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_word_count_df = df.groupby([\"language\", \"word\", \"chip\"]).speaker.agg(individual_count_per_chip_per_word=\"count\")\n",
    "total_chip_count_df = df.groupby([\"language\", \"word\"]).chip.agg(total_chips_per_word=\"count\")\n",
    "\n",
    "# frequentist probability of a chip given word and language\n",
    "p_chip_word_language = per_word_count_df[\"individual_count_per_chip_per_word\"] / total_chip_count_df[\"total_chips_per_word\"]\n",
    "p_chip_word_language.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Bottleneck\n",
    "\n",
    "Code taken from Frank (osfstorage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = ['a', 'b', 'c', 'r', 'x', 'y', 'z']\n",
    "\n",
    "# probabilities from Geoff's dissertation from Google N-grams (1985)\n",
    "allp =  np.array([0.1034, 0.0795, 0.1839, 0.6183, 0.0074, 0.0048, 0.0028])\n",
    "pastprops = allp[0:3]/np.sum(allp[0:3])\n",
    "futprops = allp[4:7]/np.sum(allp[4:7])\n",
    "duboisprobs = np.array([27.4, 47.5, 25.1]) # From Twitter Corpus\n",
    "duboisprobs = duboisprobs / np.sum(duboisprobs)\n",
    "p_x = np.concatenate( [duboisprobs[0] * pastprops, duboisprobs[[1]], duboisprobs[2]*futprops ] )\n",
    "p_x = p_x / np.sum(p_x)\n",
    "\n",
    "eps = 0.01\n",
    "q0 = (1 - eps) * np.eye(7) + eps * np.ones((7, 7))\n",
    "\n",
    "kap = 0.5\n",
    "lam = 0.1\n",
    "\n",
    "p_xGy = np.array(\n",
    "    [[1, kap, kap ** 2, lam * kap ** 2, lam ** 2 * kap ** 2, lam ** 2 * kap ** 3, lam ** 2 * kap ** 4],\n",
    "     [kap, 1, kap, lam * kap, lam ** 2 * kap, lam ** 2 * kap ** 2, lam ** 2 * kap ** 3],\n",
    "     [kap ** 2, kap, 1, lam, lam ** 2, lam ** 2 * kap, lam ** 2 * kap ** 2],\n",
    "     [lam * kap ** 2, lam * kap, lam, 1, lam, lam * kap, lam * kap ** 2],\n",
    "     [lam ** 2 * kap ** 2, lam ** 2 * kap, lam ** 2, lam, 1, kap, kap ** 2],\n",
    "     [lam ** 2 * kap ** 3, lam ** 2 * kap ** 2, lam ** 2 * kap, lam * kap, kap, 1, kap],\n",
    "     [lam ** 2 * kap ** 4, lam ** 2 * kap ** 3, lam ** 2 * kap ** 2, lam * kap ** 2, kap ** 2, kap, 1]])\n",
    "\n",
    "p_mGs = p_xGy / np.sum(p_xGy, axis=0)\n",
    "p_xGy = p_xGy / p_xGy.sum(axis=1, keepdims=True)\n",
    "p_xy = p_xGy * p_x[:, np.newaxis]\n",
    "p_xy = p_xy / np.sum(p_xy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pareto Frontier\n",
    "Now let's run the Information Bottleneck Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace out optimal frontier\n",
    "q0 = q0 / q0.sum(axis=1, keepdims=True) # q0 initial encoder - can set it to identity\n",
    "betas = np.array([2.0 ** x for x in np.arange(5, 0, -0.001)])\n",
    "focalbeta = 5.3\n",
    "\n",
    "q, beta, ibscores, qresult, qseq, qseqresults, allqs = fit_ib(p_xy, q0, focalbeta, betas, verbose=1)\n",
    "\n",
    "# create data frames for plotting and analysis\n",
    "ib_scores_df = pandas.DataFrame(np.array(ibscores), columns = ['rate', 'distortion', 'elen'])\n",
    "ib_scores_df['beta'] = betas\n",
    "ib_scores_df['q'] = allqs\n",
    "ib_scores_df['Wn'] = [mergecols(q).shape[1] for q in ib_scores_df['q']]\n",
    "\n",
    "# The structural phase transitions along the pareto frontier\n",
    "stochSys = []\n",
    "for i, q in enumerate(zip(qseq, qseqresults)):\n",
    "    for w in mergecols(q[0]).transpose():\n",
    "        stochSys.append([len(qseq)-i, q[1][0], q[1][1]] + list(w))\n",
    "\n",
    "stochSys = pandas.DataFrame(data=np.array(stochSys), columns = ['n', 'rate', 'distortion'] + items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distance from optimal frontier\n",
    "def fd(asys, ibscores):\n",
    "    mind = distance.cdist([[asys['rate'], asys['distortion']]], ibscores[['rate', 'distortion']]).min()\n",
    "    return mind\n",
    "\n",
    "\n",
    "def gNID_d(asys, paretoQs, betas, pX):\n",
    "    mind = np.zeros((len(asys), len(paretoQs)))\n",
    "    for li in range(len(asys)):\n",
    "        for qi, q in enumerate(paretoQs):\n",
    "            mind[li, qi] = gNID(asys.iloc[li]['q'], q, pX)\n",
    "    return np.argmin(mind, axis=1), np.min(mind, axis=1), betas[np.argmin(mind, axis=1)]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbda1052f39ca0b0a0cd74911b0c91c0e224307f85d570c8823d0661ac0877c0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lc': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
